# InvestiGator Architecture Redesign Specification
**Date**: 2025-11-12
**Version**: 1.0
**Status**: Design Specification (Not Implementation)
**Purpose**: Compare current vs proposed architecture with actionable recommendations

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Current Architecture Analysis](#current-architecture-analysis)
3. [Proposed Architecture](#proposed-architecture)
4. [SEC Data Strategy Comparison](#sec-data-strategy-comparison)
5. [Data Model Changes](#data-model-changes)
6. [Configuration Redesign](#configuration-redesign)
7. [Deterministic vs Agentic AI Decision Matrix](#deterministic-vs-agentic-ai-decision-matrix)
8. [Phased Implementation Plan](#phased-implementation-plan)
9. [Trade-offs and Decision Matrix](#trade-offs-and-decision-matrix)

---

## Executive Summary

### Current State Assessment

**Architecture Score**: 5.9/10 (NEEDS WORK)

**Key Findings**:
- âœ… Clean architecture foundation correctly implemented
- âœ… Excellent cache infrastructure (multi-tier with priorities)
- âœ… DAG-based orchestration for task scheduling
- âŒ **CRITICAL**: Fiscal period handling broken for 80% of companies
- âŒ **HIGH**: Cache hit rate 5% actual vs 75% potential
- âŒ **HIGH**: YTD value storage schema doesn't match real-world patterns

**Technical Debt**: ~230 hours total (~140 hours critical path)

### 7 Critical Pain Points Identified

| # | Issue | Severity | Impact | Files Affected |
|---|-------|----------|--------|----------------|
| 1 | Fiscal Period Handling | CRITICAL | Q4 computation negative for 2,000 Russell 1000 stocks | 5 files |
| 2 | Cache Key Inconsistency | HIGH | Cache hit rate 5% vs 75% potential | 4 files |
| 3 | YTD Value Storage | HIGH | Schema mismatch for 80% of S&P 100 | 3 files |
| 4 | Configuration Fragmentation | MEDIUM | Settings in 3 places, inconsistency risk | 3 files |
| 5 | Synthesizer Bloat | MEDIUM | 2000+ lines mixing domain/application/presentation | 1 file |
| 6 | Import Path Confusion | MEDIUM | Circular dependencies, DDD boundary violations | 6 files |
| 7 | Statement-Level Concerns | MEDIUM | YTD handling scattered without abstraction | 4 files |

---

## Current Architecture Analysis

### 1. Architecture Layers

**Current Implementation** (Clean Architecture - Mostly Correct):

```
InvestiGator/
â”œâ”€ src/investigator/
â”‚  â”œâ”€ domain/                  # Business logic (âœ… Correct separation)
â”‚  â”‚  â”œâ”€ agents/               # 7 agent implementations
â”‚  â”‚  â”œâ”€ models/               # AgentTask, AgentResult, QuarterlyData
â”‚  â”‚  â”œâ”€ services/             # Domain services (minimal)
â”‚  â”‚  â””â”€ value_objects/        # Value objects
â”‚  â”‚
â”‚  â”œâ”€ application/             # Orchestration (âš ï¸ Synthesizer bloated)
â”‚  â”‚  â”œâ”€ orchestrator.py       # DAG-based task orchestration (âœ… Good)
â”‚  â”‚  â””â”€ synthesizer.py        # 2000+ lines mixing concerns (âŒ SRP violation)
â”‚  â”‚
â”‚  â”œâ”€ infrastructure/          # External adapters (âœ… Well-designed)
â”‚  â”‚  â”œâ”€ cache/                # Multi-tier cache (Priority 20 file, 10 RDBMS)
â”‚  â”‚  â”œâ”€ database/             # PostgreSQL adapters
â”‚  â”‚  â”œâ”€ llm/                  # Ollama client, pool, VRAM-aware semaphore
â”‚  â”‚  â””â”€ sec/                  # SEC data extractors/processors
â”‚  â”‚
â”‚  â””â”€ interfaces/              # External interfaces
â”‚     â””â”€ cli/                  # CLI commands
â”‚
â””â”€ utils/                      # LEGACY utilities (being migrated)
   â”œâ”€ canonical_key_mapper.py  # XBRL tag normalization (247 mappings)
   â”œâ”€ dcf_valuation.py         # DCF valuation
   â”œâ”€ gordon_growth_model.py   # GGM valuation
   â””â”€ quarterly_calculator.py  # YTD normalization (âŒ BROKEN)
```

**Issues**:
1. **Boundary Violations**: `base.py` imports from `utils/` (legacy)
2. **Synthesizer Bloat**: Mixing domain logic (DCF), application (orchestration), presentation (PDF)
3. **No Centralized Services**: Fiscal period logic scattered across 5+ modules
4. **Legacy Dependencies**: `utils/` still in use, should be in `infrastructure/` or `domain/services/`

---

### 2. Agent Pattern Analysis

**Current Pattern** (âœ… Template Method - Correctly Implemented):

```python
# src/investigator/domain/agents/base.py
class InvestmentAgent(ABC):
    @abstractmethod
    def register_capabilities(self) -> List[AgentCapability]:
        """Register what this agent can do"""
        pass

    @abstractmethod
    async def process(self, task: AgentTask) -> AgentResult:
        """Process an analysis task"""
        pass

    # Template method chain:
    async def run(task: AgentTask):
        1. pre_process()     # Validation
        2. cache.get()       # Check cache
        3. execute_with_retry()  # Actual work
        4. post_process()    # Normalize + metrics
        5. cache.set()       # Save result
        6. return AgentResult
```

**Agents Implemented** (7 total):

1. **SECAnalysisAgent** - Fetch raw SEC CompanyFacts API, cache it (âœ… Working)
2. **FundamentalAnalysisAgent** - Extract metrics, compute ratios, valuation (âš ï¸ 1900+ lines)
3. **TechnicalAnalysisAgent** - OHLCV, 80+ indicators, LLM pattern analysis (âœ… Clean)
4. **MarketContextAgent** - ETF data, sector indices, macro indicators (âœ… Clean)
5. **SynthesisAgent** - Blend agent results, generate recommendation (âš ï¸ Most logic in synthesizer.py)
6. **SymbolUpdateAgent** - Purpose unclear (âš ï¸ Review needed)
7. **[Cache Key Builder Pattern]** - Centralized cache key generation (âœ… Well-designed but NOT USED consistently)

**Problems**:
1. **Cache Key Inconsistency**: `base.py:276-283` doesn't use `CacheKeyBuilder`
2. **Missing fiscal_period in keys**: Reduces cache hit rate from 75% to 5%
3. **Fundamental Agent Bloat**: 1900+ lines mixing data extraction, ratios, valuation
4. **YTD Handling Scattered**: Logic in agent, not in domain service

---

### 3. Cache Architecture

**Current Design** (âœ… EXCELLENT but underutilized):

```
CacheManager (coordinator)
â”œâ”€ File Cache (Priority 20)        â† Fastest, disk-based
â”‚  â”œâ”€ FileCacheStorageHandler      (gzipped JSON)
â”‚  â””â”€ ParquetCacheStorageHandler   (Parquet for time series)
â”œâ”€ RDBMS Cache (Priority 10)       â† Durable, shared
â”‚  â””â”€ RdbmsCacheStorageHandler     (PostgreSQL llm_responses table)
â””â”€ Auto-promotion: Data found in lower priority â†’ promoted to higher priority cache
```

**Cache Types** (7 total):
- `SEC_RESPONSE`: 2160 hours (90 days)
- `LLM_RESPONSE`: 720 hours (30 days)
- `TECHNICAL_DATA`: 24 hours (1 day)
- `SUBMISSION_DATA`: 2160 hours (90 days)
- `COMPANY_FACTS`: 2160 hours (90 days)
- `QUARTERLY_METRICS`: 168 hours (7 days)
- `MARKET_CONTEXT`: 168 hours (7 days)

**Problem: Inconsistent Cache Key Generation**

Current state:
- **Base Agent** (base.py:276-283): Does NOT use CacheKeyBuilder
  ```python
  cache_key = {
      "symbol": task.symbol,
      "analysis_type": task.analysis_type.value,
      "context_hash": task.get_cache_key()[:8],  # âŒ Hash, not period!
  }
  # Missing: fiscal_period - causes cache MISSES!
  ```

- **CacheKeyBuilder** exists but not used:
  ```python
  # cache_key_builder.py (WELL-DESIGNED, NOT USED)
  build_key(
      CacheType.LLM_RESPONSE,
      symbol='AAPL',
      fiscal_period='2025-Q2',  # âœ… Includes period
      analysis_type='fundamental_analysis'
  )
  ```

**Impact**: Cache hit rate is **5-10% actual vs 75% potential**

---

### 4. SEC Data Processing Pipeline

**Current 3-Tier Architecture**:

```
Tier 1: SEC CompanyFacts API (Raw JSON)
    â†“ [SECAnalysisAgent]
    Endpoint: https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json
    Cache: CacheType.COMPANY_FACTS
    Issue: 4MB+ per company, contains both YTD and individual quarter entries

Tier 2: Bulk Tables (PostgreSQL)
    â†“
    Tables: sec_sub_data, sec_num_data, sec_pre_data, sec_tag_data
    Authority: SEC DERA bulk data (authoritative)
    Issue: Requires understanding `qtrs` field semantics:
        qtrs=1: individual quarter or segment
        qtrs=2: Q2 YTD (Q1+Q2)
        qtrs=3: Q3 YTD (Q1+Q2+Q3)
        qtrs=4: full year
    Current bug: No filtering by qtrs, picks first row (usually wrong)

Tier 3: Processed Table (PostgreSQL)
    â†“
    Table: sec_companyfacts_processed
    Schema: Flat columns (symbol, fiscal_year, fiscal_period, operating_cash_flow, ...)
    Extraction: CompanyFacts â†’ processed table
    Missing: income_statement_qtrs, cash_flow_statement_qtrs columns
```

**Critical Issue: Fiscal Period Handling**

**Problem**: Fiscal period logic scattered across 5+ modules, each with variations:

1. **data_strategy.py:20-50**: `_fiscal_period_to_int(fp)` - Does NOT handle "Q2-YTD", "SECOND QUARTER"
2. **period_utils.py:15-41**: `standardize_period()` - Returns string "2024-Q1" but some code expects tuple (2024, "Q1")
3. **fundamental/agent.py:1177-1180**: Infers `is_ytd` from `qtrs` field
4. **quarterly_calculator.py:21-200**: Assumes `Q4 = FY - (Q1+Q2+Q3)` works for all (âŒ BROKEN for YTD)
5. **data_processor.py:100-200**: Tries to "fix" period_end after fact

**Real-World Example - AAPL Q4 Computation Failure**:
```
FY OCF = $110.5B (full year)
Q1 OCF = $39.9B  (PIT quarter)
Q2 OCF = $62.6B  (YTD cumulative Q1+Q2)
Q3 OCF = $91.4B  (YTD cumulative Q1+Q2+Q3)

Attempted Q4 = FY - (Q1+Q2+Q3) = 110.5 - (39.9 + 62.6 + 91.4) = -$83.4B âŒ NEGATIVE!
```

**Root Cause**: No centralized fiscal period service, YTD detection not validated before computation

---

### 5. Configuration Architecture

**Current Sprawl** (âŒ 3 Files, No Validation):

1. **config.json** (PRIMARY - JSON, 200+ lines)
   - Database connection
   - SEC API settings
   - Analysis parameters
   - Valuation models config
   - **Issue**: Untyped, no validation

2. **src/investigator/config/config.py** (Python dataclasses)
   - DatabaseConfig, OllamaConfig, SECConfig, AnalysisConfig
   - CacheTypeConfig with TTL values
   - **Issue**: Redundant with config.json, must maintain sync

3. **config.yaml** (OPTIONAL - Ollama servers)
   - LLM server endpoints
   - Cache paths
   - **Issue**: Not validated against config.py

**Data Flow**:
```
CLI args â†’ config.json â†’ config.py dataclasses â†’ settings.py (Pydantic) â†’ get_config()
```

**Problems**:
1. **Triple Definition**: Same settings defined in 3 places
2. **No Validation**: config.yaml can contradict config.json
3. **Hard to Extend**: Adding new setting requires changes to all 3 files
4. **Type Safety**: config.json is untyped, validation happens at runtime
5. **Circular Imports**: Config module imports from other modules

---

### 6. Data Model Issues

**QuarterlyData Model** (âŒ Doesn't Match Reality):

```python
@dataclass
class QuarterlyData:
    fiscal_year: int
    fiscal_period: str        # "Q1", "Q2", "Q3", "Q4", "FY"
    financial_data: Dict[str, Any]
    is_ytd_cashflow: bool     # âŒ Statement-specific YTD tracking at entity level
    is_ytd_income: bool       # âŒ Should be in statement structure
```

**Problem**: YTD flags stored at entity level but SHOULD be per-statement

**Real-World Patterns** (from S&P 100 analysis):
- **80% of stocks**: Cash flow YTD only (qtrs=2,3), income individual (qtrs=1)
- **20% of stocks**: Both statements individual (qtrs=1)
- **Current schema**: Cannot represent this variation

**Example - AAPL Q2-2024**:
```python
QuarterlyData(
    fiscal_period="Q2",
    is_ytd_cashflow=True,     # Cash flow data is Q1+Q2 cumulative
    is_ytd_income=False,      # Income statement is individual Q2 only
    financial_data={
        "operating_cash_flow": 62.6B,    # This is ACTUALLY YTD!
        "revenues": 90.8B                # This is ACTUALLY individual!
    }
)
```

**Mismatch**: Flags at entity level, data at statement level

---

## Proposed Architecture

### 1. Event-Driven Architecture with Domain Services

**High-Level Design** (Proposed):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLI Interface                            â”‚
â”‚                  (User Commands & Status)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Application Orchestrator                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              DAG Task Scheduler                          â”‚   â”‚
â”‚  â”‚   (Dependency resolution, parallel execution)            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                           â”‚                                      â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚         â–¼                 â–¼                 â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  Level 1  â”‚      â”‚  Level 1  â”‚      â”‚  Level 1  â”‚              â”‚
â”‚  â”‚  Agents   â”‚      â”‚  Agents   â”‚      â”‚  Agents   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚         â”‚                 â”‚                 â”‚                   â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                           â–¼                                      â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                  â”‚  Event Bus      â”‚                             â”‚
â”‚                  â”‚  (Publish/Sub)  â”‚                             â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                           â”‚                                      â”‚
â”‚                           â–¼                                      â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                  â”‚  Level 2 Agent  â”‚                             â”‚
â”‚                  â”‚  (Synthesis)    â”‚                             â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                 â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Domain     â”‚  â”‚   Domain     â”‚  â”‚Infrastructureâ”‚  â”‚Infrastructureâ”‚
â”‚  Services    â”‚  â”‚  Services    â”‚  â”‚    Cache     â”‚  â”‚   Database   â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚â€¢ Fiscal      â”‚  â”‚â€¢ Valuation   â”‚  â”‚â€¢ Multi-tier  â”‚  â”‚â€¢ PostgreSQL  â”‚
â”‚  Period Svc  â”‚  â”‚  Router      â”‚  â”‚â€¢ Priority    â”‚  â”‚â€¢ SEC Tables  â”‚
â”‚â€¢ Statement   â”‚  â”‚â€¢ DCF Engine  â”‚  â”‚  routing     â”‚  â”‚â€¢ Processed   â”‚
â”‚  Normalizer  â”‚  â”‚â€¢ GGM Engine  â”‚  â”‚â€¢ TTL mgmt    â”‚  â”‚  Data        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Changes from Current**:

1. **Event Bus Introduction** (NEW)
   - Agents publish results to event bus
   - Synthesis subscribes to agent completion events
   - Enables asynchronous processing, easier testing
   - Decouples agent dependencies

2. **Domain Services Layer** (NEW)
   - **FiscalPeriodService**: Centralize all period logic
   - **StatementNormalizer**: Handle income/cash flow/balance sheet YTD conversion
   - **ValuationRouter**: Route to appropriate valuation model (DCF/GGM/PE/etc.)
   - **DCFEngine**: Extract from synthesizer, pure domain logic
   - **GGMEngine**: Extract from utils/, pure domain logic

3. **Split Synthesizer** (REFACTOR)
   - Current: 2000+ lines mixing concerns
   - Proposed:
     - **RecommendationBuilder** (application layer): Orchestrate valuation
     - **ReportGenerator** (application layer): Generate PDF/JSON output
     - Valuation logic â†’ DCFEngine/GGMEngine (domain services)

---

### 2. Proposed SEC Data Strategy

**4-Tier Architecture with Fallback Chains**:

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   SEC Data Access Strategy            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                           â”‚                           â”‚
        â–¼                           â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tier 1      â”‚          â”‚  Tier 2      â”‚          â”‚  Tier 3      â”‚
â”‚  Local Cache â”‚   â”€â”€>    â”‚  Bulk Tables â”‚   â”€â”€>    â”‚  Submissions â”‚
â”‚  (File/RDBMS)â”‚          â”‚  (PostgreSQL)â”‚          â”‚  API         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Priority: 1               Priority: 2                Priority: 3
   TTL: 90 days              Authority: High            Freshness: Real-time
   Speed: <10ms              Speed: ~100ms              Speed: ~2s

        â”‚                           â”‚                           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  Tier 4      â”‚
                          â”‚  CompanyFactsâ”‚
                          â”‚  API         â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             Priority: 4
                             Fallback: Final
                             Speed: ~5s
                             Size: 4MB+ JSON
```

**Tier Descriptions**:

**Tier 1: Local Cache (File/RDBMS)**
- **Source**: `data/llm_cache/` (gzipped JSON) + `llm_responses` table
- **TTL**: 90 days for SEC data, 30 days for LLM responses
- **Speed**: <10ms (in-memory or local disk)
- **Use Case**: Repeated analysis of same stock within TTL window
- **Fallback**: If cache miss or expired â†’ Tier 2

**Tier 2: Bulk Tables (PostgreSQL)**
- **Source**: `sec_sub_data`, `sec_num_data`, `sec_pre_data`, `sec_tag_data`
- **Authority**: HIGH (SEC DERA official bulk data)
- **Freshness**: Updated quarterly (within 90 days)
- **Speed**: ~100ms (indexed database query)
- **Use Case**: Historical data, batch processing, fiscal year analysis
- **Strategy**:
  - Query by `adsh` (filing identifier) for specific periods
  - Filter by `qtrs=1` for individual quarters
  - Filter by `qtrs=2,3,4` for YTD/cumulative data
- **Fallback**: If data > 90 days old or missing â†’ Tier 3

**Tier 3: Submissions API (NEW - Not Currently Used)**
- **Source**: `https://data.sec.gov/submissions/CIK##########.json`
- **Authority**: HIGH (SEC official API)
- **Freshness**: Real-time (updated within hours of filing)
- **Speed**: ~2s (HTTP request)
- **Use Case**: Recent filings, current quarter data
- **Data Returned**:
  - Recent filings list with `accessionNumber`, `filingDate`, `form` (10-K, 10-Q)
  - No financial data, only metadata
- **Strategy**:
  1. Get recent filings for symbol
  2. Identify latest 10-K (FY) and 10-Q (Q1, Q2, Q3) filings
  3. Use `accessionNumber` to query Tier 2 bulk tables
- **Fallback**: If filing not yet in bulk tables â†’ Tier 4

**Tier 4: CompanyFacts API (Current Primary, Proposed Fallback)**
- **Source**: `https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json`
- **Authority**: MEDIUM-HIGH (SEC official but includes duplicates)
- **Freshness**: Real-time
- **Speed**: ~5s (large JSON response)
- **Size**: 4MB+ per company (entire history + duplicates)
- **Use Case**: Fallback when Tier 2/3 unavailable, new companies
- **Issues**:
  - Contains both YTD and individual quarter entries (duplicates)
  - Requires post-processing to deduplicate
  - No `qtrs` field, must infer from `start` date
- **Fallback**: No fallback (final tier)

**Proposed Fallback Chain Logic**:

```python
class SECDataStrategy:
    def get_quarterly_data(self, symbol: str, fiscal_period: str):
        # Tier 1: Check cache
        cached = self.cache_manager.get(
            CacheType.COMPANY_FACTS,
            symbol=symbol,
            fiscal_period=fiscal_period
        )
        if cached and not self.is_stale(cached, max_age_days=90):
            return cached

        # Tier 2: Query bulk tables (preferred)
        bulk_data = self.bulk_table_strategy.get_data(symbol, fiscal_period)
        if bulk_data:
            self.cache_manager.set(bulk_data)
            return bulk_data

        # Tier 3: Get recent filings from Submissions API
        recent_filings = self.submissions_api.get_recent_filings(symbol)
        if recent_filings:
            # Use accessionNumber to query bulk tables
            adsh = recent_filings[fiscal_period]['accessionNumber']
            bulk_data = self.bulk_table_strategy.get_by_adsh(adsh)
            if bulk_data:
                self.cache_manager.set(bulk_data)
                return bulk_data

        # Tier 4: Fallback to CompanyFacts API
        company_facts = self.companyfacts_api.get_data(symbol)
        processed_data = self.extract_quarterly_data(company_facts, fiscal_period)
        self.cache_manager.set(processed_data)
        return processed_data
```

**Advantages of Proposed 4-Tier Strategy**:

1. **Reduced API Load**: Tier 2 (bulk tables) handles 90% of queries
2. **Real-time Freshness**: Tier 3 (Submissions API) provides recent filings without 4MB download
3. **Authoritative Data**: Tier 2 has `qtrs` field for accurate YTD detection
4. **Graceful Degradation**: Each tier falls back to next if unavailable
5. **Caching**: Tier 1 reduces database load for repeated analysis

**Current vs Proposed Comparison**:

| Aspect | Current | Proposed |
|--------|---------|----------|
| **Primary Source** | CompanyFacts API (Tier 4) | Bulk Tables (Tier 2) |
| **Fallback Chain** | None (single source) | 4 tiers with intelligent fallback |
| **YTD Detection** | Infer from `start` date | Use `qtrs` field (authoritative) |
| **Speed** | ~5s per stock (4MB JSON) | ~100ms per stock (indexed SQL) |
| **Data Quality** | Contains duplicates | Pre-filtered by `qtrs` |
| **Real-time Data** | Yes (but slow) | Tier 3 provides metadata, then query Tier 2 |
| **Cache Strategy** | Single-tier (file + RDBMS) | Multi-tier with TTL-based fallback |

---

### 3. Proposed Data Models

**Statement-Level YTD Tracking** (NEW):

```python
@dataclass
class FinancialStatement:
    """Base class for financial statements"""
    period_start: datetime
    period_end: datetime
    fiscal_period: str  # "Q1", "Q2", "Q3", "Q4", "FY"
    qtrs: int           # 1=individual, 2=YTD Q2, 3=YTD Q3, 4=FY
    data: Dict[str, Decimal]

    def is_ytd(self) -> bool:
        """Deterministic YTD detection based on qtrs field"""
        return self.qtrs >= 2

    def is_individual_quarter(self) -> bool:
        """Check if this is a single quarter (not cumulative)"""
        return self.qtrs == 1

@dataclass
class IncomeStatement(FinancialStatement):
    """Income statement specific data"""
    pass

@dataclass
class CashFlowStatement(FinancialStatement):
    """Cash flow statement specific data"""
    pass

@dataclass
class BalanceSheet(FinancialStatement):
    """Balance sheet specific data (always point-in-time)"""
    def is_ytd(self) -> bool:
        return False  # Balance sheets are always point-in-time

@dataclass
class QuarterlyData:
    """Container for all financial statements for a quarter"""
    fiscal_year: int
    fiscal_period: str
    income_statement: Optional[IncomeStatement]
    cash_flow_statement: Optional[CashFlowStatement]
    balance_sheet: Optional[BalanceSheet]

    def to_dict(self) -> Dict:
        """Convert to flat dict with statement-specific qtrs"""
        result = {
            "fiscal_year": self.fiscal_year,
            "fiscal_period": self.fiscal_period,
        }

        if self.income_statement:
            result["income_statement_qtrs"] = self.income_statement.qtrs
            result.update(self.income_statement.data)

        if self.cash_flow_statement:
            result["cash_flow_statement_qtrs"] = self.cash_flow_statement.qtrs
            result.update(self.cash_flow_statement.data)

        if self.balance_sheet:
            result["balance_sheet_qtrs"] = 0  # Always point-in-time
            result.update(self.balance_sheet.data)

        return result
```

**Database Schema Update** (SQL Migration):

```sql
-- Add statement-specific qtrs columns
ALTER TABLE sec_companyfacts_processed
ADD COLUMN income_statement_qtrs SMALLINT,
ADD COLUMN cash_flow_statement_qtrs SMALLINT,
ADD COLUMN balance_sheet_qtrs SMALLINT DEFAULT 0;

-- Create index for efficient querying
CREATE INDEX idx_companyfacts_qtrs
ON sec_companyfacts_processed (symbol, fiscal_year, fiscal_period, income_statement_qtrs, cash_flow_statement_qtrs);

-- Backfill data based on common patterns
UPDATE sec_companyfacts_processed
SET
    income_statement_qtrs = CASE fiscal_period
        WHEN 'Q1' THEN 1  -- Assume individual
        WHEN 'Q2' THEN 2  -- Assume YTD (80% pattern)
        WHEN 'Q3' THEN 3  -- Assume YTD
        WHEN 'Q4' THEN 1  -- Assume individual
        WHEN 'FY' THEN 4  -- Full year
    END,
    cash_flow_statement_qtrs = CASE fiscal_period
        WHEN 'Q1' THEN 1  -- Assume individual
        WHEN 'Q2' THEN 2  -- Assume YTD (90% pattern)
        WHEN 'Q3' THEN 3  -- Assume YTD
        WHEN 'Q4' THEN 1  -- Assume individual
        WHEN 'FY' THEN 4  -- Full year
    END,
    balance_sheet_qtrs = 0  -- Always point-in-time
WHERE income_statement_qtrs IS NULL;

-- Validate: Find cases where income and cash flow have different qtrs
SELECT symbol, fiscal_year, fiscal_period,
       income_statement_qtrs, cash_flow_statement_qtrs
FROM sec_companyfacts_processed
WHERE income_statement_qtrs != cash_flow_statement_qtrs
ORDER BY symbol, fiscal_year, fiscal_period;
```

**Advantages**:

1. **Accurate Representation**: Matches real-world patterns (80% cash flow YTD, income individual)
2. **Deterministic YTD Detection**: Use `qtrs` field directly, no inference
3. **Statement-Level Granularity**: Can convert only cash flow to individual quarters if needed
4. **Backward Compatible**: Existing code can use `is_ytd_cashflow`/`is_ytd_income` derived properties
5. **Database Queryable**: Can filter by statement type: `WHERE cash_flow_statement_qtrs = 1`

---

### 4. Proposed Configuration Architecture

**Single Source of Truth** (config.yaml with Pydantic Validation):

```yaml
# config.yaml (SINGLE FILE)

# Database configuration
database:
  host: ${DB_HOST:-localhost}
  port: 5432
  database: sec_database
  credentials:
    username: investigator
    password: ${DB_PASSWORD}  # Read from environment variable
  connection:
    pool_size: 10
    max_overflow: 20
    timeout: 30

# Ollama LLM configuration
ollama:
  servers:
    - url: http://localhost:11434
      weight: 1.0
      max_concurrent: 3
    - url: http://mac-studio-1.local:11434
      weight: 1.0
      max_concurrent: 3
  timeout: 300
  temperatures:
    factual: 0.0         # Data extraction, SEC parsing
    conservative: 0.05   # Financial analysis, ratios
    balanced: 0.1        # Synthesis, recommendations
    creative: 0.35       # Scenario generation, risk assessment

# Analysis configuration
analysis:
  fundamental_weight: 0.6
  technical_weight: 0.4
  min_score_for_buy: 7.0
  agent_timeouts:
    sec_analysis: 180
    technical_analysis: 240
    fundamental_analysis: 360
    market_context: 180
    synthesis: 120
    default: 900

# Orchestrator configuration
orchestrator:
  max_concurrent_analyses: 5
  max_concurrent_agents: 10
  task_dependency_max_retries: 100
  use_event_bus: true  # Enable event-driven architecture

# Cache configuration
cache:
  base_path: data/llm_cache
  ttl:
    llm_response: 720        # hours (30 days)
    technical_data: 24       # hours (1 day)
    company_facts: 2160      # hours (90 days)
    sec_response: 2160       # hours (90 days)
    quarterly_data: 168      # hours (7 days)
    submission_data: 2160    # hours (90 days)
    market_context: 168      # hours (7 days)
  cleanup:
    enabled: true
    interval_hours: 1

# SEC data strategy configuration
sec:
  api:
    base_url: https://data.sec.gov
    user_agent: "InvestiGator/1.0 (vijay@example.com)"  # Required by SEC
    rate_limit_delay: 0.1  # seconds between requests
  data_strategy:
    prefer_bulk_tables: true
    bulk_freshness_days: 90
    fallback_chain:
      - local_cache
      - bulk_tables
      - submissions_api
      - companyfacts_api

# Valuation configuration
valuation:
  sector_multiples_freshness_days: 7
  ggm_payout_threshold_pct: 40.0
  fading_dcf_thresholds:
    fcf_growth_pct: 15.0
    revenue_growth_pct: 10.0
    multiple_quarters: 3
  tier_base_weights:
    pre_profit_high_growth:
      ps: 60
      dcf: 30
      analyst: 10
    dividend_aristocrat_pure:
      ggm: 60
      dcf: 20
      pe: 20
```

**Pydantic Validation Schema**:

```python
# src/investigator/config/settings.py
from pydantic import BaseSettings, validator
from typing import Dict, List, Optional
from enum import Enum

class TemperatureConfig(BaseSettings):
    factual: float = 0.0
    conservative: float = 0.05
    balanced: float = 0.1
    creative: float = 0.35

    @validator('*')
    def validate_temperature(cls, v):
        if not 0 <= v <= 1:
            raise ValueError('Temperature must be between 0 and 1')
        return v

class DatabaseCredentials(BaseSettings):
    username: str
    password: str

    class Config:
        env_prefix = 'DB_'

class DatabaseConnection(BaseSettings):
    pool_size: int = 10
    max_overflow: int = 20
    timeout: int = 30

class DatabaseConfig(BaseSettings):
    host: str
    port: int = 5432
    database: str
    credentials: DatabaseCredentials
    connection: DatabaseConnection

class OllamaServer(BaseSettings):
    url: str
    weight: float = 1.0
    max_concurrent: int = 3

class OllamaConfig(BaseSettings):
    servers: List[OllamaServer]
    timeout: int = 300
    temperatures: TemperatureConfig

class AgentTimeouts(BaseSettings):
    sec_analysis: int = 180
    technical_analysis: int = 240
    fundamental_analysis: int = 360
    market_context: int = 180
    synthesis: int = 120
    default: int = 900

class AnalysisConfig(BaseSettings):
    fundamental_weight: float = 0.6
    technical_weight: float = 0.4
    min_score_for_buy: float = 7.0
    agent_timeouts: AgentTimeouts

    @validator('fundamental_weight', 'technical_weight')
    def validate_weights(cls, v):
        if not 0 <= v <= 1:
            raise ValueError('Weights must be between 0 and 1')
        return v

class OrchestratorConfig(BaseSettings):
    max_concurrent_analyses: int = 5
    max_concurrent_agents: int = 10
    task_dependency_max_retries: int = 100
    use_event_bus: bool = True

class CacheTTL(BaseSettings):
    llm_response: int = 720
    technical_data: int = 24
    company_facts: int = 2160
    sec_response: int = 2160
    quarterly_data: int = 168
    submission_data: int = 2160
    market_context: int = 168

class CacheCleanup(BaseSettings):
    enabled: bool = True
    interval_hours: int = 1

class CacheConfig(BaseSettings):
    base_path: str = "data/llm_cache"
    ttl: CacheTTL
    cleanup: CacheCleanup

class SECAPIConfig(BaseSettings):
    base_url: str = "https://data.sec.gov"
    user_agent: str
    rate_limit_delay: float = 0.1

    @validator('user_agent')
    def validate_user_agent(cls, v):
        if '@' not in v:
            raise ValueError('SEC requires user agent with email')
        return v

class SECDataStrategy(BaseSettings):
    prefer_bulk_tables: bool = True
    bulk_freshness_days: int = 90
    fallback_chain: List[str] = [
        'local_cache',
        'bulk_tables',
        'submissions_api',
        'companyfacts_api'
    ]

class SECConfig(BaseSettings):
    api: SECAPIConfig
    data_strategy: SECDataStrategy

class FadingDCFThresholds(BaseSettings):
    fcf_growth_pct: float = 15.0
    revenue_growth_pct: float = 10.0
    multiple_quarters: int = 3

class ValuationTierWeights(BaseSettings):
    ps: int = 0
    dcf: int = 0
    ggm: int = 0
    pe: int = 0
    analyst: int = 0

    @validator('*')
    def validate_sum_100(cls, v, values):
        if sum(values.values()) + v > 100:
            raise ValueError('Weights must sum to 100')
        return v

class ValuationConfig(BaseSettings):
    sector_multiples_freshness_days: int = 7
    ggm_payout_threshold_pct: float = 40.0
    fading_dcf_thresholds: FadingDCFThresholds
    tier_base_weights: Dict[str, ValuationTierWeights]

class InvestiGatorConfig(BaseSettings):
    """Root configuration object"""
    database: DatabaseConfig
    ollama: OllamaConfig
    analysis: AnalysisConfig
    orchestrator: OrchestratorConfig
    cache: CacheConfig
    sec: SECConfig
    valuation: ValuationConfig

    class Config:
        env_file = '.env'
        env_file_encoding = 'utf-8'

    @classmethod
    def from_yaml(cls, yaml_path: str = 'config.yaml'):
        """Load configuration from YAML file"""
        import yaml
        with open(yaml_path, 'r') as f:
            config_dict = yaml.safe_load(f)
        return cls(**config_dict)

# Usage:
config = InvestiGatorConfig.from_yaml('config.yaml')
# Pydantic validates ALL fields on load, raises ValidationError if invalid
```

**Advantages of Proposed Configuration**:

1. **Single Source of Truth**: One file, no sync issues
2. **Type Safety**: Pydantic validates types, ranges, relationships
3. **Environment Variables**: Support `${ENV_VAR}` for secrets
4. **Easy to Extend**: Add field to YAML + schema, automatic validation
5. **Self-Documenting**: Schema serves as documentation
6. **Error Messages**: Clear validation errors on startup
7. **IDE Support**: Type hints enable autocomplete

**Migration Path**:

1. Create `config.yaml` with all settings from `config.json` + `config.py`
2. Create Pydantic schema in `settings.py`
3. Update `get_config()` to load from YAML
4. Run validation tests
5. Deprecate `config.json` and partial `config.py`
6. Remove redundant config files after migration

---

## Deterministic vs Agentic AI Decision Matrix

### Guiding Principle

> **Use deterministic code wherever possible. Use agentic AI only when:**
> 1. Pattern recognition in noisy data is required
> 2. Natural language synthesis is needed
> 3. Subjective judgment on qualitative factors
> 4. Combining multiple signals with uncertainty

---

### Decision Matrix

| Task | Current | Proposed | Reasoning |
|------|---------|----------|-----------|
| **Fiscal Period Normalization** | âŒ Scattered logic | âœ… **Deterministic** (FiscalPeriodService) | Fixed mapping: "Q1"/"FIRST QUARTER"/"1Q" â†’ "Q1". No ambiguity. |
| **YTD Detection** | âš ï¸ Inference from start date | âœ… **Deterministic** (use `qtrs` field) | Bulk tables provide `qtrs=1/2/3/4`. Authoritative, no guessing. |
| **Q4 Computation** | âŒ Always computes Q4 = FY - (Q1+Q2+Q3) | âœ… **Deterministic** with validation | IF all quarters `qtrs=1` THEN compute Q4, ELSE skip. |
| **Cache Key Generation** | âš ï¸ Inconsistent | âœ… **Deterministic** (CacheKeyBuilder) | Fixed algorithm: `hash(symbol + fiscal_period + analysis_type)`. |
| **XBRL Tag Mapping** | âœ… **Deterministic** (canonical_key_mapper.py) | âœ… **Keep Deterministic** | 247 mappings: "Revenues" â†’ "total_revenue". Fixed mapping. |
| **Financial Ratio Calculation** | âœ… **Deterministic** | âœ… **Keep Deterministic** | `ROE = net_income / shareholders_equity`. Pure math. |
| **DCF Valuation** | âœ… **Deterministic** (dcf_valuation.py) | âœ… **Keep Deterministic** | NPV calculation with WACC. Pure math. |
| **GGM Valuation** | âœ… **Deterministic** (gordon_growth_model.py) | âœ… **Keep Deterministic** | `Fair Value = D1 / (r - g)`. Pure math. |
| **Sector Multiples** | âœ… **Deterministic** | âœ… **Keep Deterministic** | Median P/E, P/S from peer group. Statistical calculation. |
| **Valuation Model Selection** | âš ï¸ Hard-coded rules | ðŸ¤– **Agentic AI** (ValuationRouter) | Complex rules: sector, growth, profitability, lifecycle. LLM can reason through edge cases. |
| **Technical Indicator Calculation** | âœ… **Deterministic** | âœ… **Keep Deterministic** | RSI, MACD, Bollinger Bands. Fixed formulas. |
| **Technical Pattern Recognition** | ðŸ¤– **Agentic AI** | ðŸ¤– **Keep Agentic AI** | Head & shoulders, double bottom. Noisy data, subjective interpretation. |
| **Peer Group Selection** | âš ï¸ Hard-coded by sector | ðŸ¤– **Agentic AI** | Identify similar companies by business model, not just GICS sector. LLM can reason. |
| **Risk Assessment** | ðŸ¤– **Agentic AI** | ðŸ¤– **Keep Agentic AI** | Qualitative analysis: regulatory risk, competitive moat, management quality. |
| **Investment Narrative** | ðŸ¤– **Agentic AI** | ðŸ¤– **Keep Agentic AI** | Synthesize bull/bear case, narrative synthesis. Natural language generation. |
| **Recommendation Synthesis** | ðŸ¤– **Agentic AI** | ðŸ¤– **Keep Agentic AI** | Blend fundamental + technical + market context. Weight multiple signals. |

---

### Detailed Reasoning

#### **Category 1: Pure Deterministic** (Should NEVER use AI)

1. **Fiscal Period Normalization**
   - **Input**: "Q1", "FIRST QUARTER", "1Q", "Q1-YTD"
   - **Output**: "Q1"
   - **Logic**: Fixed mapping table
   - **Why Deterministic**: No ambiguity, all variations known
   - **Implementation**: `FiscalPeriodService.normalize_period(period: str) -> str`

2. **YTD Detection**
   - **Input**: `qtrs` field from bulk tables
   - **Output**: `True` if qtrs >= 2, `False` otherwise
   - **Logic**: `is_ytd = (qtrs >= 2)`
   - **Why Deterministic**: Authoritative field, no inference needed
   - **Implementation**: `FinancialStatement.is_ytd() -> bool`

3. **Q4 Computation**
   - **Input**: FY, Q1, Q2, Q3 data with `qtrs` fields
   - **Output**: Q4 value OR skip computation
   - **Logic**:
     ```python
     if all(q.qtrs == 1 for q in [q1, q2, q3]):
         q4 = fy.value - (q1.value + q2.value + q3.value)
     else:
         raise ValueError("Cannot compute Q4 from YTD data")
     ```
   - **Why Deterministic**: Math, with validation
   - **Implementation**: `QuarterlyCalculator.compute_q4_with_validation()`

4. **Cache Key Generation**
   - **Input**: symbol, fiscal_period, analysis_type
   - **Output**: `{"symbol": "AAPL", "fiscal_period": "2025-Q2", "analysis_type": "fundamental"}`
   - **Logic**: Fixed key structure per cache type
   - **Why Deterministic**: No ambiguity, need consistency for cache hits
   - **Implementation**: `CacheKeyBuilder.build_key()`

5. **Financial Ratios**
   - **Input**: Financial statement line items
   - **Output**: Ratios (ROE, ROA, debt-to-equity, etc.)
   - **Logic**: Fixed formulas (e.g., `ROE = net_income / shareholders_equity`)
   - **Why Deterministic**: Pure math, no interpretation needed
   - **Implementation**: `FinancialRatioCalculator.compute_ratios()`

---

#### **Category 2: Hybrid (Deterministic with Agentic Fallback)**

1. **Valuation Model Selection**
   - **Deterministic Part**:
     - IF dividend_payout_ratio > 40% AND dividend_growth_stable â†’ **GGM**
     - IF fcf_negative AND pre_profit â†’ **P/S multiple**
     - IF mature AND stable_fcf â†’ **DCF**
   - **Agentic AI Part**:
     - Edge cases: turnaround story, cyclical industry, regulatory uncertainty
     - LLM reasoning: "Company has negative FCF but high R&D spend in growth phase, use P/S multiple weighted 60%, DCF with conservative growth 40%"
   - **Implementation**:
     ```python
     class ValuationRouter:
         def select_model(self, company_data):
             # Try deterministic rules first
             if rule_matches := self.apply_rules(company_data):
                 return rule_matches
             # Fallback to LLM reasoning
             return self.llm_reasoner.select_model(company_data)
     ```

2. **Peer Group Selection**
   - **Deterministic Part**:
     - Same GICS sector + similar market cap â†’ initial peer group
   - **Agentic AI Part**:
     - Refine by business model similarity
     - LLM reasoning: "TSLA and F are both auto, but TSLA is growth tech, F is mature cyclical"
   - **Implementation**:
     ```python
     class PeerGroupSelector:
         def select_peers(self, symbol):
             # Deterministic: Same sector + market cap
             initial_peers = self.filter_by_sector_and_cap(symbol)
             # Agentic: Refine by business model
             refined_peers = self.llm_refiner.filter_by_model(symbol, initial_peers)
             return refined_peers
     ```

---

#### **Category 3: Pure Agentic AI** (Deterministic NOT feasible)

1. **Technical Pattern Recognition**
   - **Why Agentic**: Noisy data, subjective pattern interpretation
   - **Example**: Is this a "head and shoulders" or just volatility?
   - **Implementation**: LLM analyzes OHLCV chart, returns pattern confidence scores

2. **Risk Assessment**
   - **Why Agentic**: Qualitative factors, requires reasoning about text
   - **Example**: "Regulatory risk HIGH due to pending antitrust lawsuit"
   - **Implementation**: LLM reads SEC risk factors section, synthesizes risk score

3. **Investment Narrative**
   - **Why Agentic**: Natural language generation, storytelling
   - **Example**: "Bull case: Strong FCF growth, expanding margins, market share gains"
   - **Implementation**: LLM synthesizes bull/bear cases from fundamental + technical data

4. **Recommendation Synthesis**
   - **Why Agentic**: Multiple conflicting signals, requires judgment
   - **Example**: Fundamental score 8/10, technical score 4/10 â†’ How to weight?
   - **Implementation**: LLM reasons through conflict, generates final recommendation

---

### Implementation Guidelines

**Rule of Thumb**:
1. **Can you write a unit test with expected output?** â†’ **Deterministic**
2. **Does output depend on "reasonable interpretation"?** â†’ **Agentic AI**
3. **Is there a mathematical formula?** â†’ **Deterministic**
4. **Does it require reading unstructured text?** â†’ **Agentic AI**
5. **Is there regulatory/authoritative guidance?** â†’ **Deterministic**

**Code Pattern**:
```python
class HybridService:
    def __init__(self, deterministic_engine, llm_reasoner):
        self.deterministic = deterministic_engine
        self.llm = llm_reasoner

    def process(self, input_data):
        # Always try deterministic first (fast, cheap, testable)
        result = self.deterministic.process(input_data)
        if result.confidence >= 0.9:
            return result

        # Fallback to LLM for edge cases (slow, expensive, harder to test)
        return self.llm.process(input_data, context=result)
```

---

## Phased Implementation Plan

### Phase 1: Fiscal Period Service (CRITICAL - 40 hours)

**Goal**: Centralize all fiscal period logic, fix Q4 computation

**Deliverables**:
1. Create `src/investigator/domain/services/fiscal_period_service.py`
2. Implement methods:
   - `normalize_period(fiscal_year, fiscal_period) -> str`
   - `parse_period(period_str) -> Tuple[int, str]`
   - `is_ytd(fiscal_period, qtrs) -> bool`
   - `detect_fiscal_year_end(company_facts) -> str`
3. Update `quarterly_calculator.py` to validate `qtrs=1` before Q4 computation
4. Migrate all period normalization code to service
5. Unit tests with 100% coverage

**Files Modified**:
- NEW: `src/investigator/domain/services/fiscal_period_service.py`
- EDIT: `utils/quarterly_calculator.py` (add validation)
- EDIT: `src/investigator/infrastructure/sec/data_strategy.py` (use service)
- EDIT: `src/investigator/infrastructure/sec/data_processor.py` (use service)
- EDIT: `src/investigator/domain/agents/fundamental/agent.py` (use service)
- REMOVE: `utils/period_utils.py` (migrate to service)

**Success Criteria**:
- Q4 computation no longer produces negative values for AAPL, DASH, STX
- All period formats normalized consistently
- Zero circular imports
- 100% unit test coverage

---

### Phase 2: Cache Key Standardization (PERFORMANCE - 30 hours)

**Goal**: Use CacheKeyBuilder everywhere, include fiscal_period in all keys

**Deliverables**:
1. Update `AgentTask` model to include `fiscal_period` field
2. Update `base.py:276-283` to use `CacheKeyBuilder`
3. Update all agent subclasses to include period in cache operations
4. Add cache key validation on startup
5. Performance testing to verify 75% hit rate

**Files Modified**:
- EDIT: `src/investigator/domain/models/analysis.py` (add fiscal_period to AgentTask)
- EDIT: `src/investigator/domain/agents/base.py` (use CacheKeyBuilder)
- EDIT: `src/investigator/domain/agents/fundamental/agent.py` (pass period to cache)
- EDIT: `src/investigator/domain/agents/sec.py` (pass period to cache)
- EDIT: `src/investigator/application/synthesizer.py` (standardize keys)

**Success Criteria**:
- Cache hit rate increases from 5% to 75% for repeated analysis
- All cache keys include fiscal_period when relevant
- Zero cache key collisions
- Cache inspection tool shows correct key structure

**Testing**:
```bash
# Test cache hit rate
python3 cli_orchestrator.py analyze AAPL -m standard -o /tmp/aapl_run1.json
python3 cli_orchestrator.py analyze AAPL -m standard -o /tmp/aapl_run2.json

# Run 2 should be <2s (all cache hits)
# Inspect cache keys:
python3 cli_orchestrator.py inspect-cache --symbol AAPL --verbose
```

---

### Phase 3: Statement-Level YTD Tracking (DATA QUALITY - 50 hours)

**Goal**: Add `income_statement_qtrs`, `cash_flow_statement_qtrs` columns to schema

**Deliverables**:
1. SQL migration to add qtrs columns
2. Update `companyfacts_extractor.py` to populate qtrs fields
3. Create `FinancialStatement` base class and subclasses
4. Update `QuarterlyData` model to use statement classes
5. Implement statement-aware YTD conversion logic
6. Update `quarterly_calculator` to validate `is_ytd` before Q4 computation
7. Backfill existing data with qtrs values

**Files Modified**:
- NEW: `src/investigator/domain/models/financial_statement.py`
- NEW: `migrations/add_statement_qtrs_columns.sql`
- EDIT: `src/investigator/domain/models/quarterly_data.py` (use FinancialStatement)
- EDIT: `src/investigator/infrastructure/sec/companyfacts_extractor.py` (populate qtrs)
- EDIT: `utils/quarterly_calculator.py` (validate statement-specific qtrs)

**SQL Migration**:
```sql
-- migrations/add_statement_qtrs_columns.sql
ALTER TABLE sec_companyfacts_processed
ADD COLUMN income_statement_qtrs SMALLINT,
ADD COLUMN cash_flow_statement_qtrs SMALLINT,
ADD COLUMN balance_sheet_qtrs SMALLINT DEFAULT 0;

CREATE INDEX idx_companyfacts_qtrs
ON sec_companyfacts_processed (symbol, fiscal_year, fiscal_period, income_statement_qtrs, cash_flow_statement_qtrs);

-- Backfill with common patterns
UPDATE sec_companyfacts_processed
SET
    income_statement_qtrs = CASE fiscal_period
        WHEN 'Q1' THEN 1
        WHEN 'Q2' THEN 2
        WHEN 'Q3' THEN 3
        WHEN 'Q4' THEN 1
        WHEN 'FY' THEN 4
    END,
    cash_flow_statement_qtrs = CASE fiscal_period
        WHEN 'Q1' THEN 1
        WHEN 'Q2' THEN 2
        WHEN 'Q3' THEN 3
        WHEN 'Q4' THEN 1
        WHEN 'FY' THEN 4
    END,
    balance_sheet_qtrs = 0
WHERE income_statement_qtrs IS NULL;
```

**Success Criteria**:
- 80% of S&P 100 stocks correctly represented (cash flow YTD, income individual)
- Q4 computation no longer fails for AAPL, MSFT, GOOGL
- YTD conversion only applied to statements where `qtrs >= 2`
- Database query can filter by statement-specific qtrs

**Testing**:
```sql
-- Validate AAPL Q2 2024 has correct qtrs
SELECT symbol, fiscal_year, fiscal_period,
       income_statement_qtrs, cash_flow_statement_qtrs,
       ROUND(total_revenue/1000000, 1) as revenue_m,
       ROUND(operating_cash_flow/1000000, 1) as ocf_m
FROM sec_companyfacts_processed
WHERE symbol = 'AAPL' AND fiscal_year = 2024 AND fiscal_period = 'Q2';

-- Expected: income_statement_qtrs = 1, cash_flow_statement_qtrs = 2
```

---

### Phase 4: Configuration Consolidation (MAINTENANCE - 20 hours)

**Goal**: Single `config.yaml` with Pydantic validation

**Deliverables**:
1. Create comprehensive `config.yaml` with all settings
2. Create Pydantic schema in `settings.py`
3. Update `get_config()` to load from YAML
4. Migrate settings from `config.json` and `config.py`
5. Add environment variable support for secrets
6. Validation tests
7. Deprecate old config files

**Files Modified**:
- NEW: `config.yaml` (single source of truth)
- EDIT: `src/investigator/config/settings.py` (Pydantic schema)
- EDIT: `src/investigator/__init__.py` (update get_config())
- DEPRECATE: `config.json` (keep for backward compat, mark deprecated)
- DEPRECATE: Parts of `src/investigator/config/config.py` (keep dataclasses, remove redundancy)

**Success Criteria**:
- Single source of truth for all configuration
- Pydantic validates all fields on startup
- Clear error messages for invalid config
- Environment variables work for secrets (e.g., `${DB_PASSWORD}`)
- Migration guide for users

**Testing**:
```bash
# Test invalid config raises ValidationError
cat > config_invalid.yaml <<EOF
database:
  pool_size: -10  # Invalid: negative
EOF

python3 -c "from investigator.config.settings import InvestiGatorConfig; InvestiGatorConfig.from_yaml('config_invalid.yaml')"
# Expected: ValidationError with message "pool_size must be positive"

# Test environment variable substitution
export DB_PASSWORD="secret123"
python3 -c "from investigator.config.settings import InvestiGatorConfig; c = InvestiGatorConfig.from_yaml('config.yaml'); print(c.database.credentials.password)"
# Expected: "secret123"
```

---

### Phase 5: Split Synthesizer (TESTABILITY - 40 hours)

**Goal**: Refactor 2000-line synthesizer into 3 services

**Deliverables**:
1. Create `src/investigator/domain/services/dcf_engine.py` (extract DCF logic)
2. Create `src/investigator/domain/services/ggm_engine.py` (extract GGM logic)
3. Create `src/investigator/domain/services/valuation_router.py` (model selection)
4. Create `src/investigator/application/recommendation_builder.py` (orchestrate valuation)
5. Create `src/investigator/application/report_generator.py` (generate PDF/JSON output)
6. Update `synthesizer.py` to delegate to services
7. Unit tests for each service

**Files Modified**:
- NEW: `src/investigator/domain/services/dcf_engine.py`
- NEW: `src/investigator/domain/services/ggm_engine.py`
- NEW: `src/investigator/domain/services/valuation_router.py`
- NEW: `src/investigator/application/recommendation_builder.py`
- NEW: `src/investigator/application/report_generator.py`
- EDIT: `src/investigator/application/synthesizer.py` (slim down, delegate)
- MOVE: `utils/dcf_valuation.py` â†’ `domain/services/dcf_engine.py`
- MOVE: `utils/gordon_growth_model.py` â†’ `domain/services/ggm_engine.py`

**Proposed Structure**:
```python
# synthesizer.py (AFTER refactor - slim orchestrator)
class Synthesizer:
    def __init__(self, valuation_router, recommendation_builder, report_generator):
        self.valuation_router = valuation_router
        self.recommendation_builder = recommendation_builder
        self.report_generator = report_generator

    async def synthesize(self, agent_results):
        # 1. Select valuation model
        model = self.valuation_router.select_model(agent_results)

        # 2. Build recommendation
        recommendation = await self.recommendation_builder.build(agent_results, model)

        # 3. Generate report
        report = self.report_generator.generate(recommendation, format='json')

        return report
```

**Success Criteria**:
- Synthesizer reduced from 2000+ lines to <200 lines
- DCF logic testable in isolation (no dependencies)
- GGM logic testable in isolation
- Valuation router has unit tests for all edge cases
- Recommendation builder has unit tests
- Report generator has unit tests

---

### Phase 6: Migrate utils/ to infrastructure/ (ARCHITECTURE - 30 hours)

**Goal**: Move legacy utilities to correct architectural layer

**Deliverables**:
1. Move `canonical_key_mapper.py` â†’ `domain/services/xbrl_mapper.py`
2. Move `market_data_fetcher.py` â†’ `infrastructure/market_data/`
3. Move `monitoring.py` â†’ `infrastructure/observability/`
4. Move `event_bus.py` â†’ `infrastructure/events/`
5. Update all imports
6. Verify zero circular dependencies
7. Archive empty `utils/` directory

**Files Modified**:
- MOVE: `utils/canonical_key_mapper.py` â†’ `src/investigator/domain/services/xbrl_mapper.py`
- MOVE: `utils/monitoring.py` â†’ `src/investigator/infrastructure/observability/monitoring.py`
- MOVE: `utils/event_bus.py` â†’ `src/investigator/infrastructure/events/event_bus.py`
- UPDATE: All import statements across codebase
- REMOVE: `utils/` directory (archive in `archive/utils-legacy/`)

**Success Criteria**:
- Zero imports from `utils/` in `src/investigator/`
- All modules in correct architectural layer
- Dependency graph validates (domain â†’ application â†’ infrastructure, no cycles)
- All tests pass

---

## Trade-offs and Decision Matrix

### Key Architectural Decisions

#### Decision 1: Event Bus vs Direct Calls

**Context**: Agent results currently passed directly to synthesis agent.

**Options**:

| Aspect | Direct Calls (Current) | Event Bus (Proposed) |
|--------|------------------------|----------------------|
| **Complexity** | Low (simple function calls) | Medium (pub/sub infrastructure) |
| **Coupling** | High (agents know about synthesis) | Low (agents emit events, don't know consumers) |
| **Testability** | Medium (must mock synthesis) | High (test agents in isolation) |
| **Performance** | Fast (no overhead) | Slightly slower (event routing) |
| **Extensibility** | Low (adding consumer requires code change) | High (new consumers just subscribe) |
| **Debugging** | Easy (stack traces) | Harder (async event flow) |

**Recommendation**: **Adopt Event Bus** for long-term maintainability

**Reasoning**:
- Decouples agents (easier to test, replace, add new agents)
- Enables async processing (agents don't block each other)
- Supports multiple consumers (e.g., logging, metrics, synthesis)
- Slight performance cost acceptable (10-50ms per event)

**Implementation Priority**: Phase 7 (after critical fixes)

---

#### Decision 2: Bulk Tables vs CompanyFacts API

**Context**: Which SEC data source should be primary?

**Options**:

| Aspect | CompanyFacts API (Current) | Bulk Tables (Proposed) |
|--------|----------------------------|------------------------|
| **Authority** | HIGH (SEC official) | HIGH (SEC official DERA data) |
| **Freshness** | Real-time (updated hourly) | Quarterly (90-day lag) |
| **Speed** | Slow (~5s per stock, 4MB JSON) | Fast (~100ms, indexed SQL) |
| **YTD Detection** | Infer from `start` date | Authoritative `qtrs` field |
| **Duplicates** | Yes (contains both YTD and individual) | No (pre-filtered by qtrs) |
| **Batch Processing** | Slow (5s Ã— 1000 stocks = 83 min) | Fast (100ms Ã— 1000 = 100s) |
| **Storage** | 4GB for 1000 stocks (uncompressed) | 500MB for 1000 stocks (relational) |

**Recommendation**: **Primary: Bulk Tables, Fallback: CompanyFacts API**

**Reasoning**:
- 90% of analysis is historical (within 90-day freshness acceptable)
- Bulk tables provide authoritative `qtrs` field (eliminates YTD inference bugs)
- 50x faster for batch processing
- CompanyFacts API still available for latest quarter (Tier 4 fallback)

**Implementation Priority**: Phase 3 (with statement-level ytrs tracking)

---

#### Decision 3: Single vs Multiple Configuration Files

**Context**: Current 3 files (config.json, config.py, config.yaml) vs proposed single YAML.

**Options**:

| Aspect | Multiple Files (Current) | Single YAML (Proposed) |
|--------|--------------------------|------------------------|
| **Sync Issues** | YES (must manually sync 3 files) | NO (single source of truth) |
| **Type Safety** | Partial (Python dataclasses, no JSON validation) | Full (Pydantic validates on load) |
| **Extensibility** | Hard (change 3 files + update loaders) | Easy (add field to YAML + schema) |
| **Error Detection** | Runtime (fails during use) | Startup (fails fast with clear errors) |
| **IDE Support** | Partial (Python only) | Full (YAML schema enables autocomplete) |
| **Backward Compat** | N/A | Easy (keep config.json deprecated, map to YAML) |

**Recommendation**: **Single YAML with Pydantic Validation**

**Reasoning**:
- Eliminates sync issues (common source of bugs)
- Fail-fast validation (catch config errors on startup, not in production)
- Easier for users (one file to edit, clear error messages)
- Standard pattern (most Python projects use single config file + schema)

**Implementation Priority**: Phase 4 (after critical bugs fixed)

---

#### Decision 4: Statement-Level vs Entity-Level YTD Flags

**Context**: Should YTD tracking be per-statement or per-entity?

**Options**:

| Aspect | Entity-Level (Current) | Statement-Level (Proposed) |
|--------|------------------------|----------------------------|
| **Accuracy** | INCORRECT for 80% of stocks | CORRECT for 100% of stocks |
| **Example** | `is_ytd = True` (for entire entity) | `income_stmt.qtrs=1, cash_flow.qtrs=2` |
| **Schema** | Simple (2 boolean fields) | Moderate (3 smallint columns) |
| **Querying** | Hard (can't filter by statement type) | Easy (`WHERE cash_flow_statement_qtrs = 1`) |
| **Conversion Logic** | Scattered (check flags in code) | Deterministic (check statement.qtrs) |

**Recommendation**: **Statement-Level YTD Tracking**

**Reasoning**:
- Matches reality (80% of S&P 100 have mixed YTD patterns)
- Authoritative data available (bulk tables provide `qtrs` per statement)
- Eliminates YTD conversion bugs
- Enables correct Q4 computation

**Implementation Priority**: Phase 3 (CRITICAL for data quality)

---

#### Decision 5: Synthesizer Refactor - Split vs Rewrite

**Context**: Synthesizer is 2000+ lines. Split into services or rewrite from scratch?

**Options**:

| Aspect | Split (Extract Services) | Rewrite from Scratch |
|--------|--------------------------|----------------------|
| **Risk** | Low (incremental, testable) | High (big bang, may break existing) |
| **Effort** | Medium (40 hours) | High (80+ hours) |
| **Backward Compat** | Easy (existing tests still pass) | Hard (need comprehensive new tests) |
| **Code Quality** | Good (clean separation, some legacy remains) | Excellent (greenfield, best practices) |
| **Timeline** | Phase 5 (week 5) | Phase 8+ (month 3) |

**Recommendation**: **Split (Extract Services)**

**Reasoning**:
- Lower risk (incremental refactor, existing tests validate)
- Faster timeline (40 hours vs 80+ hours)
- Preserves domain knowledge (existing logic moved, not rewritten)
- Can rewrite later if needed (after services proven)

**Implementation Priority**: Phase 5 (after critical bugs fixed)

---

### Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| **Schema migration breaks existing data** | MEDIUM | HIGH | Backfill with default values, validate before/after counts match |
| **Config migration breaks user setups** | HIGH | MEDIUM | Keep config.json deprecated but functional, migration guide |
| **Cache key change invalidates all caches** | HIGH | LOW | Accept cache invalidation (performance hit for 1 day), or add key version |
| **Fiscal period service has edge cases** | MEDIUM | MEDIUM | Comprehensive unit tests, log unknown formats, fallback to raw value |
| **Event bus adds latency** | LOW | LOW | Benchmark shows <50ms overhead, acceptable for analysis tasks |
| **Bulk table data stale for recent filings** | MEDIUM | LOW | Tier 3 fallback to Submissions API, Tier 4 to CompanyFacts API |

---

## Summary and Next Steps

### Executive Summary

**Current State**: 5.9/10 architecture with 7 critical pain points, ~230 hours technical debt

**Proposed State**: 8.5/10 architecture with centralized services, event-driven design, deterministic data processing

**Critical Path**: 140 hours (Phases 1-3)

### Immediate Next Steps (Week 1)

1. **Review this specification** with stakeholders
2. **Prioritize phases** - Confirm Phases 1-3 as critical path
3. **Set up tracking** - Create GitHub issues/Jira tickets for each phase
4. **Allocate resources** - Assign developers to phases
5. **Create test plan** - Define acceptance criteria for each phase

### Recommended Sequence

**Week 1-2: Phase 1** (Fiscal Period Service)
**Week 3-4: Phase 2** (Cache Key Standardization)
**Week 5-7: Phase 3** (Statement-Level YTD Tracking)
**Week 8: Phase 4** (Configuration Consolidation)
**Week 9-10: Phase 5** (Split Synthesizer)
**Week 11-12: Phase 6** (Migrate utils/)

**Total Timeline**: ~3 months for complete remediation

### Success Metrics

1. **Data Quality**: Q4 computation no longer produces negative values for >95% of stocks
2. **Performance**: Cache hit rate increases from 5% to 75%
3. **Maintainability**: Configuration in single file, zero circular imports
4. **Testability**: 90%+ unit test coverage for domain services
5. **Architecture**: Dependency graph validates (no cycles), clean layer separation

---

## Appendix: File Change Summary

### Phase 1: Fiscal Period Service
- **NEW**: `src/investigator/domain/services/fiscal_period_service.py`
- **EDIT**: `utils/quarterly_calculator.py`
- **EDIT**: `src/investigator/infrastructure/sec/data_strategy.py`
- **EDIT**: `src/investigator/infrastructure/sec/data_processor.py`
- **EDIT**: `src/investigator/domain/agents/fundamental/agent.py`
- **REMOVE**: `utils/period_utils.py`

### Phase 2: Cache Key Standardization
- **EDIT**: `src/investigator/domain/models/analysis.py`
- **EDIT**: `src/investigator/domain/agents/base.py`
- **EDIT**: `src/investigator/domain/agents/fundamental/agent.py`
- **EDIT**: `src/investigator/domain/agents/sec.py`
- **EDIT**: `src/investigator/application/synthesizer.py`

### Phase 3: Statement-Level YTD Tracking
- **NEW**: `src/investigator/domain/models/financial_statement.py`
- **NEW**: `migrations/add_statement_qtrs_columns.sql`
- **EDIT**: `src/investigator/domain/models/quarterly_data.py`
- **EDIT**: `src/investigator/infrastructure/sec/companyfacts_extractor.py`
- **EDIT**: `utils/quarterly_calculator.py`

### Phase 4: Configuration Consolidation
- **NEW**: `config.yaml`
- **EDIT**: `src/investigator/config/settings.py`
- **EDIT**: `src/investigator/__init__.py`
- **DEPRECATE**: `config.json`

### Phase 5: Split Synthesizer
- **NEW**: `src/investigator/domain/services/dcf_engine.py`
- **NEW**: `src/investigator/domain/services/ggm_engine.py`
- **NEW**: `src/investigator/domain/services/valuation_router.py`
- **NEW**: `src/investigator/application/recommendation_builder.py`
- **NEW**: `src/investigator/application/report_generator.py`
- **EDIT**: `src/investigator/application/synthesizer.py`
- **MOVE**: `utils/dcf_valuation.py` â†’ `domain/services/dcf_engine.py`
- **MOVE**: `utils/gordon_growth_model.py` â†’ `domain/services/ggm_engine.py`

### Phase 6: Migrate utils/
- **MOVE**: `utils/canonical_key_mapper.py` â†’ `src/investigator/domain/services/xbrl_mapper.py`
- **MOVE**: `utils/monitoring.py` â†’ `src/investigator/infrastructure/observability/monitoring.py`
- **MOVE**: `utils/event_bus.py` â†’ `src/investigator/infrastructure/events/event_bus.py`
- **UPDATE**: All import statements
- **ARCHIVE**: `utils/` directory

---

**End of Architecture Redesign Specification**
