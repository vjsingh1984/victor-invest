#!/usr/bin/env python3
"""
InvestiGator - Cache Manager
Copyright (c) 2025 Vijaykumar Singh
Licensed under the Apache License 2.0

Cache manager for coordinating multiple cache handlers
"""

import asyncio
import logging
import time
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta
from pathlib import Path
from threading import Lock
from typing import Any, Dict, List, Optional, Tuple, Union

from .cache_base import CacheStorageHandler
from .cache_types import CacheType
from .file_cache_handler import FileCacheStorageHandler
from .parquet_cache_handler import ParquetCacheStorageHandler
from .rdbms_cache_handler import RdbmsCacheStorageHandler

# Memory cache handler removed - using only disk and RDBMS

logger = logging.getLogger(__name__)

# ============================================================================
# Cache Schema Version (M8 Cache Improvements)
# ============================================================================
# Increment MINOR for backwards-compatible changes
# Increment MAJOR when cache entries from previous versions should be invalidated
CACHE_SCHEMA_VERSION = "2.0.0"

# Minimum compatible versions by cache type
MINIMUM_COMPATIBLE_VERSIONS = {
    CacheType.LLM_RESPONSE: "2.0.0",
    CacheType.SEC_RESPONSE: "2.0.0",
    CacheType.COMPANY_FACTS: "2.0.0",
    CacheType.TECHNICAL_DATA: "1.0.0",  # More tolerant for technical data
    CacheType.QUARTERLY_METRICS: "2.0.0",
    CacheType.SUBMISSION_DATA: "2.0.0",
}


class CacheManager:
    """Manager for coordinating multiple cache handlers with comprehensive performance tracking"""

    def __init__(self, config=None):
        self.handlers: Dict[CacheType, List[CacheStorageHandler]] = {}
        self.config = config

        # Performance tracking
        self._stats_lock = Lock()
        self._operation_stats = defaultdict(
            lambda: {
                "hits": 0,
                "misses": 0,
                "writes": 0,
                "errors": 0,
                "total_time_ms": 0.0,
                "avg_time_ms": 0.0,
                "handler_performance": defaultdict(
                    lambda: {"hits": 0, "misses": 0, "writes": 0, "errors": 0, "total_time_ms": 0.0}
                ),
            }
        )

        # Recent operations for debugging (last 100 per cache type)
        self._recent_operations = defaultdict(lambda: deque(maxlen=100))

        # Phase 4: Cache cleanup service (TTL enforcement)
        self._cleanup_service = None

        # FIX Issue #3: Thread pool for async I/O offloading
        # Prevents blocking the event loop during disk/DB operations
        self._executor = ThreadPoolExecutor(
            max_workers=4,
            thread_name_prefix="cache_io"
        )
        logger.debug("Cache manager initialized with 4-thread I/O pool")

        # FIX Issue #5: Per-instance cache overrides
        # Prevents global config mutation from affecting concurrent runs
        self._force_refresh_override = False
        self._force_refresh_symbols_override = None

        self._initialize_default_handlers()

    async def start_cleanup_service(self, interval_seconds: int = 3600):
        """
        Start background cache cleanup service (Phase 4.1)

        Args:
            interval_seconds: Cleanup interval in seconds (default: 1 hour)
        """
        from .cache_cleaner import CacheCleanupService

        if self._cleanup_service and self._cleanup_service.running:
            logger.warning("Cache cleanup service already running")
            return

        self._cleanup_service = CacheCleanupService(self, interval_seconds)
        await self._cleanup_service.start()
        logger.info(f"Cache cleanup service started (interval: {interval_seconds}s)")

    async def stop_cleanup_service(self):
        """Stop background cache cleanup service"""
        if self._cleanup_service:
            await self._cleanup_service.stop()
            logger.info("Cache cleanup service stopped")

    def get_cleanup_stats(self) -> Dict:
        """Get cache cleanup statistics"""
        if self._cleanup_service:
            return self._cleanup_service.get_stats()
        return {"status": "not_running"}

    def _initialize_default_handlers(self):
        """Initialize cache handlers based on detailed configuration"""
        # Try to get config object if available
        config = self.config
        if not config:
            try:
                from investigator.config import get_config

                config = get_config()
            except:
                pass

        if not config or not hasattr(config, "cache_control"):
            logger.warning("No cache configuration found - using minimal defaults")
            return

        cache_control = config.cache_control

        # If no storage configured, no caching at all
        if not cache_control.use_cache:
            logger.info("Cache disabled - all operations will be on-the-fly")
            return

        # Register handlers for each cache type based on configuration
        for cache_type in CacheType:
            cache_type_name = cache_type.value
            cache_config = cache_control.get_cache_config(cache_type_name)

            if not cache_config or not cache_config.enabled:
                logger.debug(f"Cache disabled for {cache_type_name}")
                continue

            logger.debug(f"Initializing cache handlers for {cache_type_name}")

            # Initialize disk handler if enabled
            if cache_config.disk.enabled:
                try:
                    disk_settings = cache_config.disk.settings
                    base_path = Path(disk_settings.get("base_path", f"data/{cache_type_name}_cache"))

                    # Use Parquet for technical data (tabular time-series), file handler for others
                    if cache_type == CacheType.TECHNICAL_DATA:
                        handler = ParquetCacheStorageHandler(
                            cache_type, base_path, priority=cache_config.disk.priority, config=config
                        )
                    else:
                        # MARKET_CONTEXT uses FileCacheStorageHandler (JSON) for nested dict data
                        handler = FileCacheStorageHandler(
                            cache_type, base_path, priority=cache_config.disk.priority, config=config
                        )
                    self.register_handler(cache_type, handler)
                    logger.debug(
                        f"Registered disk handler for {cache_type_name} with priority {cache_config.disk.priority}"
                    )
                except Exception as e:
                    logger.error(f"Failed to initialize disk handler for {cache_type_name}: {e}")

            # Initialize RDBMS handler if enabled
            if cache_config.rdbms.enabled:
                try:
                    rdbms_handler = RdbmsCacheStorageHandler(cache_type, priority=cache_config.rdbms.priority)
                    self.register_handler(cache_type, rdbms_handler)
                    logger.debug(
                        f"Registered RDBMS handler for {cache_type_name} with priority {cache_config.rdbms.priority}"
                    )
                except Exception as e:
                    logger.debug(f"RDBMS handler not available for {cache_type_name}: {e}")

    def register_handler(self, cache_type: CacheType, handler: CacheStorageHandler):
        """Register a cache handler"""
        if cache_type not in self.handlers:
            self.handlers[cache_type] = []
        self.handlers[cache_type].append(handler)
        # Sort by priority (highest first)
        self.handlers[cache_type].sort(key=lambda h: h.priority, reverse=True)

    def get(self, cache_type: CacheType, key: Union[Tuple, Dict]) -> Optional[Dict[str, Any]]:
        """
        Get data from cache, trying handlers in priority order (highest priority first)
        This ensures disk cache (higher priority) is checked before database (lower priority)

        Args:
            cache_type: Type of cache
            key: Cache key

        Returns:
            Cached data if found, None otherwise
        """
        operation_start = time.time()
        key_str = self._format_key_for_logging(key)

        # Check if caching is enabled
        if self.config and hasattr(self.config, "cache_control"):
            if not self.config.cache_control.use_cache or not self.config.cache_control.read_from_cache:
                logger.debug(f"üö´ Cache READ disabled globally for {cache_type.value}")
                return None

            if not self.config.cache_control.is_cache_type_enabled(cache_type.value):
                logger.debug(f"üö´ Cache READ disabled for type: {cache_type.value}")
                return None

            # FIX Issue #5: Check force refresh using instance override
            symbol = self._extract_symbol_from_key(key) if isinstance(key, (tuple, dict)) else None
            if self._should_force_refresh(symbol):
                logger.debug(f"üîÑ Force refresh enabled for {symbol or 'all'}, skipping cache")
                return None

        handlers = self.handlers.get(cache_type, [])
        if not handlers:
            logger.warning(f"‚ö†Ô∏è No cache handlers configured for {cache_type.value}")
            return None

        # Track attempted handlers for logging
        handler_attempts = []

        for handler in handlers:
            if handler.priority < 0:
                continue  # Skip handlers with negative priority (audit-only)

            handler_start = time.time()
            handler_name = handler.__class__.__name__

            try:
                result = handler.get(key)
                handler_time = (time.time() - handler_start) * 1000

                handler_attempts.append(
                    {
                        "handler": handler_name,
                        "priority": handler.priority,
                        "time_ms": handler_time,
                        "result": "HIT" if result is not None else "MISS",
                    }
                )

                if result is not None:
                    total_time = (time.time() - operation_start) * 1000

                    # Calculate TTL remaining for enhanced logging
                    ttl_remaining = self._calculate_ttl_remaining(result, cache_type)

                    # Check if cache has expired - treat as miss if TTL expired
                    if ttl_remaining == "expired":
                        logger.info(
                            f"‚è∞ Cache EXPIRED [{handler_name}]: {cache_type.value} | Key: {key_str} | Priority: {handler.priority} | Time: {handler_time:.1f}ms"
                        )
                        self._update_handler_stats(cache_type, handler_name, "miss", handler_time)
                        continue  # Try next handler or return None (cache miss)

                    # Check for empty LLM response - treat as corrupted cache (cache miss)
                    if cache_type == CacheType.LLM_RESPONSE and isinstance(result, dict):
                        response_data = result.get("response", result)
                        if not response_data or (isinstance(response_data, dict) and not response_data):
                            logger.warning(
                                f"‚ö†Ô∏è  Cache EMPTY [{handler_name}]: {cache_type.value} | Key: {key_str} | Treating as cache miss"
                            )
                            self._update_handler_stats(cache_type, handler_name, "miss", handler_time)
                            continue  # Try next handler or return None (cache miss)

                    # For LLM responses, add the specific type for better visibility
                    ttl_info = ttl_remaining
                    if cache_type == CacheType.LLM_RESPONSE:
                        llm_type = self._determine_llm_response_type(result)
                        ttl_info = f"{ttl_remaining} ({llm_type})"

                    # Enhanced logging with key info and TTL
                    logger.info(
                        f"‚úÖ Cache HIT [{handler_name}]: {cache_type.value} | Key: {key_str} | Priority: {handler.priority} | Time: {handler_time:.1f}ms | Total: {total_time:.1f}ms | TTL: {ttl_info}"
                    )

                    # Update statistics
                    self._update_stats(cache_type, "hit", total_time, handler_name, handler_time)

                    # Log operation details
                    self._log_operation(cache_type, "GET_HIT", key_str, handler_name, total_time)

                    # Promote to higher priority if applicable
                    self._promote_to_higher_priority(cache_type, key, result, handler.priority)

                    return result
                else:
                    logger.debug(
                        f"‚ùå Cache MISS [{handler_name}]: {cache_type.value} | Key: {key_str} | Priority: {handler.priority} | Time: {handler_time:.1f}ms"
                    )
                    self._update_handler_stats(cache_type, handler_name, "miss", handler_time)

            except Exception as e:
                handler_time = (time.time() - handler_start) * 1000
                logger.error(
                    f"üí• Cache ERROR [{handler_name}]: {cache_type.value} | Key: {key_str} | Error: {e} | Time: {handler_time:.1f}ms"
                )
                self._update_stats(cache_type, "error", handler_time, handler_name, handler_time)
                handler_attempts.append(
                    {
                        "handler": handler_name,
                        "priority": handler.priority,
                        "time_ms": handler_time,
                        "result": f"ERROR: {str(e)[:50]}",
                    }
                )
                continue

        total_time = (time.time() - operation_start) * 1000

        # Comprehensive miss logging
        handler_summary = " | ".join([f"{h['handler']}({h['priority']}):{h['result']}" for h in handler_attempts])
        logger.info(
            f"‚ùå Cache MISS ALL: {cache_type.value} | Key: {key_str} | Handlers: {handler_summary} | Total: {total_time:.1f}ms"
        )

        # Update statistics
        self._update_stats(cache_type, "miss", total_time)
        self._log_operation(cache_type, "GET_MISS", key_str, "ALL_HANDLERS", total_time)

        return None

    def set(self, cache_type: CacheType, key: Union[Tuple, Dict], value: Dict[str, Any]) -> bool:
        """
        Set data in cache handlers based on priority.

        If data exists in higher priority cache, skip writing to lower priority.
        If data doesn't exist or needs updating, write to all handlers.
        File cache will overwrite, RDBMS will upsert.

        Args:
            cache_type: Type of cache
            key: Cache key
            value: Data to cache

        Returns:
            True if at least one handler succeeded
        """
        operation_start = time.time()
        key_str = self._format_key_for_logging(key)

        # Check if caching is enabled
        if self.config and hasattr(self.config, "cache_control"):
            if not self.config.cache_control.use_cache or not self.config.cache_control.write_to_cache:
                logger.debug(f"üö´ Cache WRITE disabled globally for {cache_type.value}")
                return False

            if not self.config.cache_control.is_cache_type_enabled(cache_type.value):
                logger.debug(f"üö´ Cache WRITE disabled for type: {cache_type.value}")
                return False

        handlers = self.handlers.get(cache_type, [])
        if not handlers:
            logger.warning(f"‚ö†Ô∏è No cache handlers configured for {cache_type.value}")
            return False

        success_count = 0
        total_handlers = len(handlers)
        write_attempts = []

        # Check if data already exists in any handler (in priority order)
        existing_priority = -1
        existing_handler = None

        for handler in handlers:
            if handler.exists(key):
                existing_priority = handler.priority
                existing_handler = handler.__class__.__name__
                logger.debug(f"üìÅ Cache EXISTS in [{existing_handler}] with priority {handler.priority}")
                break

        # Write to handlers based on existence check
        for handler in handlers:
            handler_name = handler.__class__.__name__

            try:
                # Skip writing to lower priority handlers if data exists in higher priority
                if existing_priority > 0 and handler.priority < existing_priority:
                    logger.debug(
                        f"‚è© Cache SKIP WRITE [{handler_name}]: exists in higher priority cache ({existing_handler})"
                    )
                    write_attempts.append(
                        {
                            "handler": handler_name,
                            "priority": handler.priority,
                            "result": "SKIPPED",
                            "reason": f"exists_in_{existing_handler}",
                        }
                    )
                    continue

                # Always write to handlers - file will overwrite, RDBMS will upsert
                handler_start = time.time()
                success = handler.set(key, value)
                handler_time = (time.time() - handler_start) * 1000

                write_attempts.append(
                    {
                        "handler": handler_name,
                        "priority": handler.priority,
                        "time_ms": handler_time,
                        "result": "SUCCESS" if success else "FAILED",
                    }
                )

                if success:
                    success_count += 1
                    logger.info(
                        f"‚úÖ Cache WRITE SUCCESS [{handler_name}]: {cache_type.value} | Key: {key_str} | Time: {handler_time:.1f}ms"
                    )
                    self._update_stats(cache_type, "write", handler_time, handler_name, handler_time)
                else:
                    # Handler returned False - could be intentional skip (no DataFrame, missing identifiers)
                    # Log as DEBUG instead of WARNING since these are often expected behaviors
                    logger.debug(
                        f"‚è≠Ô∏è  Cache WRITE SKIPPED [{handler_name}]: {cache_type.value} | Key: {key_str} | Time: {handler_time:.1f}ms"
                    )
                    self._update_handler_stats(cache_type, handler_name, "skip", handler_time)

            except Exception as e:
                handler_time = (time.time() - handler_start) if "handler_start" in locals() else 0
                handler_time_ms = handler_time * 1000
                logger.error(
                    f"üí• Cache WRITE ERROR [{handler_name}]: {cache_type.value} | Key: {key_str} | Error: {e} | Time: {handler_time_ms:.1f}ms"
                )

                write_attempts.append(
                    {
                        "handler": handler_name,
                        "priority": handler.priority,
                        "time_ms": handler_time_ms,
                        "result": f"ERROR: {str(e)[:50]}",
                    }
                )

                self._update_stats(cache_type, "error", handler_time_ms, handler_name, handler_time_ms)

        total_time = (time.time() - operation_start) * 1000

        # Comprehensive write result logging
        handler_summary = " | ".join([f"{a['handler']}({a['priority']}):{a['result']}" for a in write_attempts])

        if success_count > 0:
            logger.info(
                f"‚úÖ Cache WRITE COMPLETE: {cache_type.value} | Key: {key_str} | Success: {success_count}/{total_handlers} | Handlers: {handler_summary} | Total: {total_time:.1f}ms"
            )
        else:
            # All handlers skipped - log as DEBUG if no exceptions occurred, WARNING if errors
            has_errors = any("ERROR" in status for status in handler_summary.split("|"))
            log_level = logger.warning if has_errors else logger.debug
            log_level(
                f"‚è≠Ô∏è  Cache WRITE SKIPPED ALL: {cache_type.value} | Key: {key_str} | Handlers: {handler_summary} | Total: {total_time:.1f}ms"
            )

        # Log operation details
        operation_result = "WRITE_SUCCESS" if success_count > 0 else "WRITE_FAILED"
        self._log_operation(
            cache_type, operation_result, key_str, f"{success_count}/{total_handlers}_handlers", total_time
        )

        return success_count > 0

    # ========================================================================
    # FIX Issue #3: Async Wrappers for Non-Blocking Cache Operations
    # ========================================================================

    async def get_async(self, cache_type: CacheType, key: Union[Tuple, Dict]) -> Optional[Dict[str, Any]]:
        """
        Async wrapper for get() - offloads blocking I/O to thread pool.

        This prevents cache reads from blocking the event loop, allowing
        concurrent agent execution even during slow disk/DB operations.

        Args:
            cache_type: Type of cache
            key: Cache key

        Returns:
            Cached data if found, None otherwise

        Example:
            # Non-blocking cache read in async agent
            cached = await cache_manager.get_async(CacheType.LLM_RESPONSE, cache_key)
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self._executor,
            self.get,  # Existing sync implementation
            cache_type,
            key
        )

    async def set_async(self, cache_type: CacheType, key: Union[Tuple, Dict], value: Dict[str, Any]) -> bool:
        """
        Async wrapper for set() - offloads blocking I/O to thread pool.

        This prevents cache writes from blocking the event loop, allowing
        concurrent agent execution even during slow disk/DB operations.

        Args:
            cache_type: Type of cache
            key: Cache key
            value: Data to cache

        Returns:
            True if at least one handler succeeded

        Example:
            # Non-blocking cache write in async agent
            await cache_manager.set_async(CacheType.LLM_RESPONSE, cache_key, result)
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self._executor,
            self.set,  # Existing sync implementation
            cache_type,
            key,
            value
        )

    def shutdown(self):
        """
        Shutdown the cache manager and cleanup resources.

        Ensures thread pool executor is properly closed to prevent
        resource leaks and allow graceful application shutdown.

        Should be called when the application terminates or when
        the cache manager is no longer needed.
        """
        if hasattr(self, '_executor') and self._executor:
            logger.info("Shutting down cache I/O thread pool...")
            self._executor.shutdown(wait=True, cancel_futures=False)
            logger.debug("Cache I/O thread pool shutdown complete")

    def __del__(self):
        """Cleanup on garbage collection"""
        try:
            if hasattr(self, '_executor') and self._executor:
                self._executor.shutdown(wait=False, cancel_futures=True)
        except:
            pass  # Ignore errors during cleanup

    # ========================================================================
    # End of Async Wrappers
    # ========================================================================

    # ========================================================================
    # FIX Issue #5: Cache Override Methods
    # ========================================================================

    def set_force_refresh(self, force_refresh: bool, symbols: Optional[List[str]] = None):
        """
        Set per-instance force refresh override (Issue #5 fix).

        Instead of mutating the global config singleton, this sets an
        instance-level override that only affects this cache manager.

        Args:
            force_refresh: Enable/disable force refresh for this instance
            symbols: Optional list of symbols to force refresh (None = all)

        Example:
            cache_manager = get_cache_manager()
            cache_manager.set_force_refresh(True, ['AAPL', 'MSFT'])
            # Only this instance bypasses cache for AAPL/MSFT
            try:
                result = await analyze('AAPL')
            finally:
                cache_manager.clear_overrides()
        """
        self._force_refresh_override = force_refresh
        self._force_refresh_symbols_override = symbols
        logger.debug(f"Set force_refresh override: {force_refresh}, symbols: {symbols}")

    def clear_overrides(self):
        """
        Clear all cache overrides for this instance (Issue #5 fix).

        Should be called in a finally block to ensure overrides don't
        leak to subsequent operations.

        Example:
            cache_manager.set_force_refresh(True, ['AAPL'])
            try:
                result = await analyze('AAPL')
            finally:
                cache_manager.clear_overrides()  # Ensure cleanup
        """
        self._force_refresh_override = False
        self._force_refresh_symbols_override = None
        logger.debug("Cleared cache overrides")

    def _should_force_refresh(self, symbol: Optional[str] = None) -> bool:
        """
        Check if cache should be bypassed (force refresh).

        Checks both instance overrides (priority) and global config (fallback).

        Args:
            symbol: Optional symbol to check for symbol-specific override

        Returns:
            True if cache should be bypassed for this symbol
        """
        # Check instance override first (higher priority)
        if self._force_refresh_override:
            # If no specific symbols, force refresh all
            if self._force_refresh_symbols_override is None:
                return True
            # Check if this symbol is in the override list
            if symbol and symbol in self._force_refresh_symbols_override:
                return True

        # Fall back to global config
        if self.config and hasattr(self.config, "cache_control"):
            if self.config.cache_control.force_refresh:
                return True
            if (symbol and self.config.cache_control.force_refresh_symbols
                and symbol in self.config.cache_control.force_refresh_symbols):
                return True

        return False

    # ========================================================================
    # End of Cache Overrides
    # ========================================================================

    def _promote_to_higher_priority(
        self, cache_type: CacheType, key: Union[Tuple, Dict], value: Dict[str, Any], found_priority: int
    ):
        """
        Promote data to higher priority storage handlers when found in lower priority storage
        This ensures frequently accessed data migrates to faster storage (disk)

        Args:
            cache_type: Type of cache
            key: Cache key
            value: Data to promote
            found_priority: Priority of handler where data was found
        """
        handlers = self.handlers.get(cache_type, [])

        for handler in handlers:
            # Only promote to handlers with higher priority than where we found the data
            if handler.priority > found_priority and handler.priority >= 0:
                try:
                    if not handler.exists(key):
                        if handler.set(key, value):
                            logger.debug(
                                f"Cache PROMOTED [{handler.__class__.__name__}]: {cache_type.value} from priority {found_priority} to {handler.priority}"
                            )
                except Exception as e:
                    logger.warning(f"Cache promotion error [{handler.__class__.__name__}]: {e}")

    def exists(self, cache_type: CacheType, key: Union[Tuple, Dict]) -> bool:
        """Check if key exists in any handler"""
        operation_start = time.time()
        key_str = self._format_key_for_logging(key)
        handlers = self.handlers.get(cache_type, [])

        if not handlers:
            logger.warning(f"‚ö†Ô∏è No cache handlers configured for {cache_type.value}")
            return False

        existence_checks = []

        for handler in handlers:
            if handler.priority < 0:
                continue

            handler_start = time.time()
            handler_name = handler.__class__.__name__

            try:
                exists = handler.exists(key)
                handler_time = (time.time() - handler_start) * 1000

                existence_checks.append(
                    {"handler": handler_name, "priority": handler.priority, "time_ms": handler_time, "exists": exists}
                )

                if exists:
                    total_time = (time.time() - operation_start) * 1000
                    logger.info(
                        f"üìÅ Cache EXISTS [{handler_name}]: {cache_type.value} | Key: {key_str} | Priority: {handler.priority} | Time: {handler_time:.1f}ms | Total: {total_time:.1f}ms"
                    )
                    return True
                else:
                    logger.debug(
                        f"üìÇ Cache NOT EXISTS [{handler_name}]: {cache_type.value} | Key: {key_str} | Priority: {handler.priority} | Time: {handler_time:.1f}ms"
                    )

            except Exception as e:
                handler_time = (time.time() - handler_start) * 1000
                logger.error(
                    f"üí• Cache EXISTS ERROR [{handler_name}]: {cache_type.value} | Key: {key_str} | Error: {e} | Time: {handler_time:.1f}ms"
                )
                existence_checks.append(
                    {
                        "handler": handler_name,
                        "priority": handler.priority,
                        "time_ms": handler_time,
                        "exists": False,
                        "error": str(e)[:50],
                    }
                )

        total_time = (time.time() - operation_start) * 1000

        # Summary logging
        handler_summary = " | ".join(
            [f"{c['handler']}({c['priority']}):{'EXISTS' if c['exists'] else 'NOT_EXISTS'}" for c in existence_checks]
        )
        logger.info(
            f"üìÇ Cache NOT EXISTS ALL: {cache_type.value} | Key: {key_str} | Handlers: {handler_summary} | Total: {total_time:.1f}ms"
        )

        return False

    def delete(self, cache_type: CacheType, key: Union[Tuple, Dict]) -> bool:
        """Delete from all handlers"""
        handlers = self.handlers.get(cache_type, [])
        any_deleted = False

        for handler in handlers:
            try:
                if handler.delete(key):
                    any_deleted = True
            except Exception as e:
                logger.warning(f"Cache delete error [{handler.__class__.__name__}]: {e}")

        return any_deleted

    def delete_by_symbol(self, symbol: str, cache_types: Optional[List[CacheType]] = None) -> Dict[str, int]:
        """
        Delete all cache entries for a specific symbol across specified cache types.
        Optimized for symbol-based cleanup using targeted deletion methods.

        Args:
            symbol: Stock symbol to delete (e.g., 'AAPL')
            cache_types: List of cache types to clean, or None for all types

        Returns:
            Dictionary with cache_type -> deleted_count mapping
        """
        operation_start = time.time()
        symbol = symbol.upper()  # Normalize to uppercase

        # Use all cache types if none specified
        target_cache_types = cache_types or list(self.handlers.keys())

        deletion_results = {}
        total_deleted = 0

        logger.info(f"üßπ Starting symbol-based cleanup for {symbol} across {len(target_cache_types)} cache types")

        for cache_type in target_cache_types:
            handlers = self.handlers.get(cache_type, [])
            if not handlers:
                continue

            cache_type_deleted = 0
            handler_results = []

            for handler in handlers:
                handler_start = time.time()
                handler_name = handler.__class__.__name__

                try:
                    # Use optimized symbol-based deletion
                    if hasattr(handler, "delete_by_symbol"):
                        deleted_count = handler.delete_by_symbol(symbol)
                    else:
                        # Fallback to pattern-based deletion for compatibility
                        deleted_count = handler.delete_by_pattern(f"*{symbol}*")

                    handler_time = (time.time() - handler_start) * 1000
                    cache_type_deleted += deleted_count

                    handler_results.append({"handler": handler_name, "deleted": deleted_count, "time_ms": handler_time})

                    if deleted_count > 0:
                        logger.info(
                            f"üóëÔ∏è Symbol cleanup [{handler_name}]: {cache_type.value} | Symbol: {symbol} | Deleted: {deleted_count} | Time: {handler_time:.1f}ms"
                        )
                    else:
                        logger.debug(
                            f"üîç Symbol cleanup [{handler_name}]: {cache_type.value} | Symbol: {symbol} | No entries found | Time: {handler_time:.1f}ms"
                        )

                except Exception as e:
                    handler_time = (time.time() - handler_start) * 1000
                    logger.error(
                        f"üí• Symbol cleanup ERROR [{handler_name}]: {cache_type.value} | Symbol: {symbol} | Error: {e} | Time: {handler_time:.1f}ms"
                    )
                    handler_results.append(
                        {"handler": handler_name, "deleted": 0, "time_ms": handler_time, "error": str(e)}
                    )

            deletion_results[cache_type.value] = cache_type_deleted
            total_deleted += cache_type_deleted

            # Summary logging for this cache type
            if cache_type_deleted > 0:
                handler_summary = " | ".join(
                    [f"{r['handler']}:{r['deleted']}" for r in handler_results if r["deleted"] > 0]
                )
                logger.info(
                    f"‚úÖ Symbol cleanup COMPLETE: {cache_type.value} | Symbol: {symbol} | Total deleted: {cache_type_deleted} | Handlers: {handler_summary}"
                )

        total_time = (time.time() - operation_start) * 1000

        # Final summary
        if total_deleted > 0:
            cache_summary = " | ".join([f"{ct}:{count}" for ct, count in deletion_results.items() if count > 0])
            logger.info(
                f"üéØ Symbol cleanup SUCCESS: Symbol: {symbol} | Total deleted: {total_deleted} | Cache types: {cache_summary} | Total time: {total_time:.1f}ms"
            )
        else:
            logger.info(
                f"üîç Symbol cleanup COMPLETE: Symbol: {symbol} | No entries found across all cache types | Total time: {total_time:.1f}ms"
            )

        return deletion_results

    def delete_by_pattern(self, cache_type: CacheType, pattern: str) -> int:
        """Delete all cache entries matching a pattern across all handlers (legacy method)"""
        total_deleted = 0
        handlers = self.handlers.get(cache_type, [])

        for handler in handlers:
            try:
                deleted_count = handler.delete_by_pattern(pattern)
                total_deleted += deleted_count
            except Exception as e:
                logger.error(f"Handler {handler.__class__.__name__} failed to delete by pattern: {e}")

        logger.info(f"Total deleted by pattern '{pattern}' from {cache_type}: {total_deleted}")
        return total_deleted

    def clear_cache_type(self, cache_type: CacheType) -> bool:
        """Clear all data for a specific cache type across all handlers"""
        all_success = True
        handlers = self.handlers.get(cache_type, [])

        for handler in handlers:
            try:
                success = handler.clear_all()
                if not success:
                    all_success = False
            except Exception as e:
                logger.error(f"Handler {handler.__class__.__name__} failed to clear: {e}")
                all_success = False

        logger.info(f"Cleared cache type {cache_type}: {'success' if all_success else 'partial/failed'}")
        return all_success

    def clear_all_caches(self) -> bool:
        """Clear all data from all cache types and handlers"""
        all_success = True

        for cache_type in self.handlers.keys():
            success = self.clear_cache_type(cache_type)
            if not success:
                all_success = False

        logger.info(f"Cleared all caches: {'success' if all_success else 'partial/failed'}")
        return all_success

    def _format_key_for_logging(self, key: Union[Tuple, Dict]) -> str:
        """Format cache key for logging purposes"""
        if isinstance(key, dict):
            if "symbol" in key:
                return f"symbol:{key['symbol']}"
            return f"dict:{len(key)}keys"
        elif isinstance(key, tuple):
            if len(key) > 0:
                return f"tuple:{key[0]}"
            return "tuple:empty"
        else:
            return str(key)[:50]

    def _extract_symbol_from_key(self, key: Union[Tuple, Dict]) -> Optional[str]:
        """Extract symbol from cache key for logging and filtering"""
        if isinstance(key, tuple) and len(key) > 0:
            return key[0]
        elif isinstance(key, dict):
            return key.get("symbol")
        return None

    def _update_stats(
        self,
        cache_type: CacheType,
        operation: str,
        total_time: float,
        handler_name: str = None,
        handler_time: float = None,
    ):
        """Update performance statistics"""
        with self._stats_lock:
            stats = self._operation_stats[cache_type.value]

            if operation == "hit":
                stats["hits"] += 1
            elif operation == "miss":
                stats["misses"] += 1
            elif operation == "write":
                stats["writes"] += 1
            elif operation == "error":
                stats["errors"] += 1

            stats["total_time_ms"] += total_time
            total_ops = stats["hits"] + stats["misses"] + stats["writes"]
            if total_ops > 0:
                stats["avg_time_ms"] = stats["total_time_ms"] / total_ops

            if handler_name and handler_time is not None:
                self._update_handler_stats(cache_type, handler_name, operation, handler_time)

    def _update_handler_stats(self, cache_type: CacheType, handler_name: str, operation: str, handler_time: float):
        """Update per-handler performance statistics"""
        # NOTE: No lock needed here - always called from within _update_stats which already holds the lock
        handler_stats = self._operation_stats[cache_type.value]["handler_performance"][handler_name]

        if operation == "hit":
            handler_stats["hits"] += 1
        elif operation == "miss":
            handler_stats["misses"] += 1
        elif operation == "write":
            handler_stats["writes"] += 1
        elif operation == "error":
            handler_stats["errors"] += 1

        handler_stats["total_time_ms"] += handler_time

    def _log_operation(self, cache_type: CacheType, operation: str, key_str: str, handler: str, time_ms: float):
        """Log detailed operation information for debugging"""
        operation_info = {
            "timestamp": time.time(),
            "operation": operation,
            "key": key_str,
            "handler": handler,
            "time_ms": time_ms,
        }

        with self._stats_lock:
            self._recent_operations[cache_type.value].append(operation_info)

    def get_performance_stats(self) -> Dict[str, Any]:
        """Get comprehensive performance statistics"""
        with self._stats_lock:
            stats = {}
            for cache_type, type_stats in self._operation_stats.items():
                # Calculate hit ratio
                hits = type_stats["hits"]
                misses = type_stats["misses"]
                total_reads = hits + misses
                hit_ratio = (hits / total_reads * 100) if total_reads > 0 else 0

                stats[cache_type] = {
                    "operations": {
                        "hits": hits,
                        "misses": misses,
                        "writes": type_stats["writes"],
                        "errors": type_stats["errors"],
                        "total_reads": total_reads,
                    },
                    "performance": {
                        "hit_ratio_pct": round(hit_ratio, 2),
                        "avg_time_ms": round(type_stats["avg_time_ms"], 2),
                        "total_time_ms": round(type_stats["total_time_ms"], 2),
                    },
                    "handlers": {},
                }

                # Handler-specific stats
                for handler_name, handler_stats in type_stats["handler_performance"].items():
                    handler_total_reads = handler_stats["hits"] + handler_stats["misses"]
                    handler_hit_ratio = (
                        (handler_stats["hits"] / handler_total_reads * 100) if handler_total_reads > 0 else 0
                    )

                    stats[cache_type]["handlers"][handler_name] = {
                        "hits": handler_stats["hits"],
                        "misses": handler_stats["misses"],
                        "writes": handler_stats["writes"],
                        "errors": handler_stats["errors"],
                        "hit_ratio_pct": round(handler_hit_ratio, 2),
                        "total_time_ms": round(handler_stats["total_time_ms"], 2),
                    }

            return stats

    def get_recent_operations(self, cache_type: CacheType = None, limit: int = 20) -> Dict[str, Any]:
        """Get recent cache operations for debugging"""
        with self._stats_lock:
            if cache_type:
                operations = list(self._recent_operations[cache_type.value])[-limit:]
                return {cache_type.value: operations}
            else:
                result = {}
                for ct, ops in self._recent_operations.items():
                    result[ct] = list(ops)[-limit:]
                return result

    def get_stats(self) -> Dict[str, Any]:
        """Get comprehensive cache statistics"""
        stats = {"cache_types": {}, "total_handlers": 0, "handler_summary": {}}

        for cache_type, handlers in self.handlers.items():
            cache_type_stats = {"handler_count": len(handlers), "handlers": []}

            for handler in handlers:
                handler_stats = {
                    "handler_type": handler.__class__.__name__,
                    "priority": handler.priority,
                    "cache_type": handler.cache_type.value,
                }

                # Get handler-specific stats if available
                if hasattr(handler, "get_stats"):
                    try:
                        handler_stats.update(handler.get_stats())
                    except Exception as e:
                        handler_stats["stats_error"] = str(e)

                cache_type_stats["handlers"].append(handler_stats)
                stats["total_handlers"] += 1

                # Update handler summary
                handler_name = handler.__class__.__name__
                if handler_name not in stats["handler_summary"]:
                    stats["handler_summary"][handler_name] = 0
                stats["handler_summary"][handler_name] += 1

            stats["cache_types"][cache_type.value] = cache_type_stats

        return stats

    async def ping(self) -> bool:
        """
        Health check for cache system - tests if cache is operational.

        Returns:
            bool: True if cache system is responsive, False otherwise
        """
        try:
            # Test if we have any handlers registered
            if not self.handlers:
                logger.warning("Cache ping: No handlers registered")
                return False

            # Try a simple exists check on a test key (doesn't create data)
            # Using LLM_RESPONSE as it's commonly registered
            test_cache_type = CacheType.LLM_RESPONSE
            if test_cache_type not in self.handlers:
                # Fall back to first available cache type
                test_cache_type = next(iter(self.handlers.keys()))

            test_key = {"_ping_test": "health_check", "timestamp": time.time()}

            # Just check if the system can process a request without errors
            # We don't actually expect this key to exist
            _ = self.exists(test_cache_type, test_key)

            return True

        except Exception as e:
            logger.error(f"Cache ping failed: {e}")
            return False

    def _calculate_ttl_remaining(self, cached_data: Any, cache_type: CacheType) -> Optional[str]:
        """
        Calculate TTL remaining for cached data based on cache type and metadata.

        Args:
            cached_data: The cached data that may contain metadata
            cache_type: Type of cache for TTL rules

        Returns:
            String representation of TTL remaining (e.g., "2h 30m", "expired", "no_ttl")
        """
        try:
            # Default TTL rules by cache type (in hours)
            default_ttls = {
                CacheType.SEC_RESPONSE: 6,  # 6 hours for SEC API responses
                CacheType.COMPANY_FACTS: 90 * 24,  # 90 days for company facts
                CacheType.TECHNICAL_DATA: 7 * 24,  # 7 days for technical data
                CacheType.SUBMISSION_DATA: 90 * 24,  # 90 days for submission data
                CacheType.QUARTERLY_METRICS: 24,  # 24 hours for quarterly metrics
            }

            # Get TTL for this cache type
            ttl_hours = default_ttls.get(cache_type, 24)  # Default to 24 hours

            # Special handling for LLM_RESPONSE - determine type from metadata
            if cache_type == CacheType.LLM_RESPONSE:
                llm_type = self._determine_llm_response_type(cached_data)
                if llm_type == "fundamental":
                    ttl_hours = 30 * 24  # 30 days for fundamental analysis
                elif llm_type == "technical":
                    ttl_hours = 7 * 24  # 7 days for technical analysis
                elif llm_type == "market_context":
                    ttl_hours = 10  # 10 hours for market context data
                elif llm_type == "synthesis":
                    ttl_hours = 7 * 24  # 7 days for synthesis
                else:
                    ttl_hours = 24  # Default 24 hours for unknown LLM types

            # Extract metadata to find cached_at timestamp
            cached_at_str = None
            timestamp_field_names = ["cached_at", "fetched_at", "created_at", "timestamp"]

            # Check different metadata structures
            if isinstance(cached_data, dict):
                # Check direct metadata
                if "metadata" in cached_data:
                    metadata = cached_data["metadata"]
                    if isinstance(metadata, dict):
                        for field_name in timestamp_field_names:
                            cached_at_str = metadata.get(field_name)
                            if cached_at_str:
                                break

                # Check nested response structure if not found yet
                if not cached_at_str and "response" in cached_data and isinstance(cached_data["response"], dict):
                    response_metadata = cached_data["response"].get("metadata", {})
                    if isinstance(response_metadata, dict):
                        for field_name in timestamp_field_names:
                            cached_at_str = response_metadata.get(field_name)
                            if cached_at_str:
                                break

                # Check if timestamp is directly in the data if not found yet
                if not cached_at_str:
                    for field_name in timestamp_field_names:
                        cached_at_str = cached_data.get(field_name)
                        if cached_at_str:
                            break

            if not cached_at_str:
                return "no_ttl"

            # Parse the cached_at timestamp
            try:
                # Ensure we have a string
                if not isinstance(cached_at_str, str):
                    logger.debug(f"Timestamp is not a string: {type(cached_at_str)} = {cached_at_str}")
                    return "parse_error"

                # Handle empty string
                if not cached_at_str.strip():
                    logger.debug("Timestamp is empty string")
                    return "parse_error"

                # Handle both with and without microseconds
                if "." in cached_at_str:
                    cached_at = datetime.fromisoformat(cached_at_str.replace("Z", "+00:00"))
                else:
                    cached_at = datetime.fromisoformat(cached_at_str.replace("Z", ""))

                # If timezone-naive, assume UTC
                if cached_at.tzinfo is None:
                    from datetime import timezone

                    cached_at = cached_at.replace(tzinfo=timezone.utc)

                # Calculate age and remaining TTL
                now = datetime.now(cached_at.tzinfo)
                age = now - cached_at
                ttl_delta = timedelta(hours=ttl_hours)
                remaining = ttl_delta - age

                if remaining.total_seconds() <= 0:
                    return "expired"

                # Format remaining time nicely
                total_seconds = int(remaining.total_seconds())
                hours = total_seconds // 3600
                minutes = (total_seconds % 3600) // 60

                if hours > 0:
                    if minutes > 0:
                        return f"{hours}h {minutes}m"
                    else:
                        return f"{hours}h"
                elif minutes > 0:
                    return f"{minutes}m"
                else:
                    return f"{total_seconds}s"

            except (ValueError, TypeError) as e:
                logger.debug(f"Error parsing cached_at timestamp '{cached_at_str}': {e}")
                return "parse_error"

        except Exception as e:
            logger.debug(f"Error calculating TTL: {e}")
            return "calc_error"

    def _determine_llm_response_type(self, cached_data: Any) -> str:
        """
        Determine the type of LLM response (fundamental, technical, synthesis) from cached data.

        Args:
            cached_data: The cached LLM response data

        Returns:
            String indicating the LLM response type: 'fundamental', 'technical', 'synthesis', or 'unknown'
        """
        try:
            # Check various locations where the LLM type might be stored

            # Method 1: Check cache key metadata
            if isinstance(cached_data, dict):
                # Check metadata.cache_key.llm_type
                metadata = cached_data.get("metadata", {})
                if isinstance(metadata, dict):
                    cache_key = metadata.get("cache_key", {})
                    if isinstance(cache_key, dict):
                        llm_type = cache_key.get("llm_type", "")
                        if llm_type == "sec":
                            return "fundamental"
                        elif llm_type == "ta":
                            return "technical"
                        elif llm_type == "full":
                            return "synthesis"
                        elif llm_type:
                            return llm_type  # Return as-is if it's something else

                # Method 2: Check response metadata
                response = cached_data.get("response", {})
                if isinstance(response, dict):
                    response_metadata = response.get("metadata", {})
                if isinstance(response_metadata, dict):
                    # Check task_type
                    task_type = response_metadata.get("task_type", "")
                    if "fundamental" in task_type.lower() or "sec" in task_type.lower():
                        return "fundamental"
                    elif "technical" in task_type.lower() or "ta" in task_type.lower():
                        return "technical"
                    elif "market" in task_type.lower():
                        return "market_context"
                    elif "synthesis" in task_type.lower() or "investment" in task_type.lower():
                        return "synthesis"

                    # Check analysis_type
                    analysis_type = response_metadata.get("analysis_type", "")
                    if "fundamental" in analysis_type.lower():
                        return "fundamental"
                    elif "technical" in analysis_type.lower():
                        return "technical"
                    elif "market" in analysis_type.lower():
                        return "market_context"
                    elif "synthesis" in analysis_type.lower():
                        return "synthesis"

                # Method 3: Check direct form_type in cache key
                form_type = cached_data.get("form_type", "")
                if form_type == "COMPREHENSIVE" or form_type.startswith("10-"):
                    return "fundamental"
                elif form_type == "TECHNICAL":
                    return "technical"
                elif form_type == "SYNTHESIS":
                    return "synthesis"

                # Method 4: Heuristic based on content structure
                if "financial_health_score" in str(cached_data) or "quarterly_summary" in str(cached_data):
                    return "fundamental"
                elif "technical_score" in str(cached_data) or "rsi" in str(cached_data).lower():
                    return "technical"
                elif "investment_thesis" in str(cached_data) or "overall_score" in str(cached_data):
                    return "synthesis"

            return "unknown"

        except Exception as e:
            logger.debug(f"Error determining LLM response type: {e}")
            return "unknown"

    # ========================================================================
    # M8: Cache Version Tagging and Validation
    # ========================================================================

    def create_cache_metadata(
        self,
        cache_type: CacheType,
        key: Union[Tuple, Dict],
        additional_metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Create standardized cache metadata with version tagging.

        This ensures all cache entries have consistent metadata including:
        - Schema version for compatibility checking
        - Timestamp for TTL calculations
        - Key information for debugging
        - Optional additional context

        Args:
            cache_type: Type of cache being written
            key: Cache key (tuple or dict)
            additional_metadata: Optional extra metadata to include

        Returns:
            Dict with standardized cache metadata

        Example:
            metadata = cache_manager.create_cache_metadata(
                CacheType.LLM_RESPONSE,
                {'symbol': 'AAPL', 'llm_type': 'fundamental'},
                {'model': 'qwen2.5:14b'}
            )
        """
        now = datetime.now()

        metadata = {
            "schema_version": CACHE_SCHEMA_VERSION,
            "cached_at": now.isoformat(),
            "cached_at_unix": now.timestamp(),
            "cache_type": cache_type.value,
            "cache_key": self._normalize_key_for_metadata(key),
        }

        # Add symbol if available
        symbol = self._extract_symbol_from_key(key)
        if symbol:
            metadata["symbol"] = symbol

        # Merge additional metadata
        if additional_metadata:
            metadata.update(additional_metadata)

        return metadata

    def _normalize_key_for_metadata(self, key: Union[Tuple, Dict]) -> Dict[str, Any]:
        """Convert cache key to dict format for metadata storage."""
        if isinstance(key, dict):
            return key.copy()
        elif isinstance(key, tuple):
            # Convert tuple to dict with positional keys
            result = {}
            if len(key) >= 1:
                result["symbol"] = key[0]
            if len(key) >= 2:
                result["llm_type"] = key[1]
            if len(key) >= 3:
                result["fiscal_period"] = key[2]
            return result
        else:
            return {"raw_key": str(key)}

    def validate_cache_entry(
        self,
        entry: Dict[str, Any],
        cache_type: CacheType,
        strict: bool = False
    ) -> Tuple[bool, List[str]]:
        """
        Validate a cache entry for integrity and compatibility.

        Checks performed:
        1. Schema version compatibility
        2. Required fields present
        3. Data structure validity
        4. TTL not exceeded (if strict mode)

        Args:
            entry: Cache entry to validate
            cache_type: Expected cache type
            strict: If True, also check TTL and data quality

        Returns:
            Tuple of (is_valid, list_of_issues)

        Example:
            is_valid, issues = cache_manager.validate_cache_entry(
                cached_data,
                CacheType.LLM_RESPONSE,
                strict=True
            )
            if not is_valid:
                logger.warning(f"Invalid cache entry: {issues}")
        """
        issues = []

        if not isinstance(entry, dict):
            return (False, ["Entry is not a dictionary"])

        # Check schema version
        metadata = entry.get("metadata", entry)
        schema_version = None

        if isinstance(metadata, dict):
            schema_version = metadata.get("schema_version")

        if schema_version:
            min_version = MINIMUM_COMPATIBLE_VERSIONS.get(cache_type, "1.0.0")
            if not self._is_version_compatible(schema_version, min_version):
                issues.append(
                    f"Schema version {schema_version} < minimum {min_version}"
                )
        else:
            # No version = legacy cache, may need migration
            issues.append("Missing schema_version (legacy cache entry)")

        # Check for empty data
        if cache_type == CacheType.LLM_RESPONSE:
            response_data = entry.get("response", entry)
            if not response_data:
                issues.append("Empty response data")
            elif isinstance(response_data, dict) and not response_data:
                issues.append("Empty response dictionary")

        # Check for required timestamp
        if isinstance(metadata, dict):
            has_timestamp = any(
                metadata.get(field) for field in
                ["cached_at", "fetched_at", "created_at", "timestamp"]
            )
            if not has_timestamp:
                issues.append("Missing timestamp field")

        # Strict mode checks
        if strict:
            # Check TTL
            ttl_remaining = self._calculate_ttl_remaining(entry, cache_type)
            if ttl_remaining == "expired":
                issues.append("Cache entry has expired")

            # Check data quality for fundamental data
            if cache_type == CacheType.LLM_RESPONSE:
                quality_issues = self._check_data_quality(entry)
                issues.extend(quality_issues)

        is_valid = len(issues) == 0
        return (is_valid, issues)

    def _is_version_compatible(
        self,
        version: str,
        minimum: str
    ) -> bool:
        """
        Check if version is >= minimum version.

        Uses semantic versioning comparison (MAJOR.MINOR.PATCH).
        """
        try:
            def parse_version(v: str) -> Tuple[int, int, int]:
                parts = v.split(".")
                major = int(parts[0]) if len(parts) > 0 else 0
                minor = int(parts[1]) if len(parts) > 1 else 0
                patch = int(parts[2]) if len(parts) > 2 else 0
                return (major, minor, patch)

            current = parse_version(version)
            min_ver = parse_version(minimum)

            return current >= min_ver

        except (ValueError, AttributeError):
            # If parsing fails, assume incompatible
            return False

    def _check_data_quality(self, entry: Dict[str, Any]) -> List[str]:
        """
        Check data quality issues in cache entry.

        Returns list of quality issues found.
        """
        issues = []

        # Check for None/null values in critical fields
        critical_fields = ["fair_value", "valuation", "analysis", "metrics"]

        def check_nulls(data: Any, path: str = ""):
            if isinstance(data, dict):
                for key, value in data.items():
                    full_path = f"{path}.{key}" if path else key
                    if key in critical_fields and value is None:
                        issues.append(f"Null value in critical field: {full_path}")
                    elif isinstance(value, (dict, list)):
                        check_nulls(value, full_path)
            elif isinstance(data, list):
                for i, item in enumerate(data):
                    check_nulls(item, f"{path}[{i}]")

        check_nulls(entry)

        return issues

    # ========================================================================
    # M8: Auto-Invalidation on SEC Updates
    # ========================================================================

    def invalidate_on_sec_update(
        self,
        symbol: str,
        new_filing_date: str,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        Invalidate cache entries when new SEC filing is detected.

        When a new SEC filing is available (10-K, 10-Q), this method:
        1. Finds all cached entries for the symbol
        2. Checks if they were cached before the new filing
        3. Invalidates (deletes) stale entries
        4. Returns summary of what was invalidated

        Args:
            symbol: Stock symbol (e.g., 'AAPL')
            new_filing_date: Date of new SEC filing (ISO format: '2024-11-15')
            dry_run: If True, only report what would be invalidated

        Returns:
            Dict with invalidation summary:
            {
                'symbol': 'AAPL',
                'new_filing_date': '2024-11-15',
                'entries_checked': 15,
                'entries_invalidated': 8,
                'entries_kept': 7,
                'invalidated_by_type': {'llm_response': 5, 'quarterly_metrics': 3},
                'dry_run': False
            }

        Example:
            # New 10-Q filed for AAPL
            result = cache_manager.invalidate_on_sec_update(
                'AAPL',
                '2024-11-15',
                dry_run=True  # Preview first
            )
            print(f"Would invalidate {result['entries_invalidated']} entries")
        """
        symbol = symbol.upper()
        operation_start = time.time()

        result = {
            "symbol": symbol,
            "new_filing_date": new_filing_date,
            "entries_checked": 0,
            "entries_invalidated": 0,
            "entries_kept": 0,
            "invalidated_by_type": {},
            "kept_by_type": {},
            "dry_run": dry_run,
            "errors": [],
        }

        try:
            # Parse the new filing date
            filing_date = datetime.fromisoformat(new_filing_date.split("T")[0])
        except ValueError as e:
            result["errors"].append(f"Invalid filing date format: {e}")
            return result

        # Cache types that should be invalidated on SEC update
        sec_sensitive_types = [
            CacheType.LLM_RESPONSE,
            CacheType.SEC_RESPONSE,
            CacheType.COMPANY_FACTS,
            CacheType.QUARTERLY_METRICS,
        ]

        logger.info(
            f"{'[DRY RUN] ' if dry_run else ''}Checking SEC invalidation for {symbol} "
            f"(new filing: {new_filing_date})"
        )

        for cache_type in sec_sensitive_types:
            handlers = self.handlers.get(cache_type, [])
            type_name = cache_type.value

            for handler in handlers:
                handler_name = handler.__class__.__name__

                try:
                    # Get all entries for this symbol
                    if hasattr(handler, "list_entries_for_symbol"):
                        entries = handler.list_entries_for_symbol(symbol)
                    else:
                        # Fallback: can't enumerate entries
                        continue

                    for entry_key, entry_metadata in entries:
                        result["entries_checked"] += 1

                        # Check if entry was cached before new filing
                        cached_at = self._extract_cached_at(entry_metadata)

                        if cached_at and cached_at < filing_date:
                            # Entry is stale
                            if not dry_run:
                                try:
                                    handler.delete(entry_key)
                                except Exception as del_err:
                                    result["errors"].append(
                                        f"Failed to delete {entry_key}: {del_err}"
                                    )
                                    continue

                            result["entries_invalidated"] += 1
                            result["invalidated_by_type"][type_name] = (
                                result["invalidated_by_type"].get(type_name, 0) + 1
                            )

                            logger.debug(
                                f"{'Would invalidate' if dry_run else 'Invalidated'} "
                                f"[{handler_name}] {type_name}: {entry_key} "
                                f"(cached: {cached_at.date()}, filing: {filing_date.date()})"
                            )
                        else:
                            result["entries_kept"] += 1
                            result["kept_by_type"][type_name] = (
                                result["kept_by_type"].get(type_name, 0) + 1
                            )

                except Exception as e:
                    result["errors"].append(
                        f"Error processing {handler_name}/{type_name}: {e}"
                    )
                    logger.error(f"SEC invalidation error: {e}")

        total_time = (time.time() - operation_start) * 1000

        action = "Would invalidate" if dry_run else "Invalidated"
        logger.info(
            f"{'[DRY RUN] ' if dry_run else ''}{action} {result['entries_invalidated']} / "
            f"{result['entries_checked']} entries for {symbol} | Time: {total_time:.1f}ms"
        )

        return result

    def _extract_cached_at(self, metadata: Any) -> Optional[datetime]:
        """Extract cached_at datetime from metadata."""
        if not isinstance(metadata, dict):
            return None

        timestamp_fields = ["cached_at", "fetched_at", "created_at", "timestamp"]

        for field in timestamp_fields:
            value = metadata.get(field)
            if value:
                try:
                    if isinstance(value, datetime):
                        return value
                    elif isinstance(value, (int, float)):
                        return datetime.fromtimestamp(value)
                    elif isinstance(value, str):
                        return datetime.fromisoformat(value.replace("Z", "+00:00"))
                except (ValueError, TypeError):
                    continue

        return None

    # ========================================================================
    # M8: Corruption Detection and Purge
    # ========================================================================

    def detect_and_purge_corrupted(
        self,
        symbol: Optional[str] = None,
        dry_run: bool = True
    ) -> Dict[str, Any]:
        """
        Detect and optionally purge corrupted cache entries.

        Corruption indicators:
        1. Empty response data
        2. Missing required fields
        3. Invalid schema version
        4. Malformed JSON structure
        5. Null values in critical fields

        Args:
            symbol: Optional symbol to check (None = check all)
            dry_run: If True, only report corruption (default: True)

        Returns:
            Dict with corruption report:
            {
                'symbol': 'AAPL' or 'ALL',
                'entries_scanned': 100,
                'corrupted_entries': 5,
                'corruption_types': {'empty_response': 3, 'missing_fields': 2},
                'purged': False,
                'dry_run': True,
                'details': [...]
            }

        Example:
            # Scan for corruption (preview mode)
            report = cache_manager.detect_and_purge_corrupted('AAPL')
            if report['corrupted_entries'] > 0:
                print(f"Found {report['corrupted_entries']} corrupted entries")
                # Purge them
                cache_manager.detect_and_purge_corrupted('AAPL', dry_run=False)
        """
        operation_start = time.time()

        result = {
            "symbol": symbol.upper() if symbol else "ALL",
            "entries_scanned": 0,
            "corrupted_entries": 0,
            "healthy_entries": 0,
            "corruption_types": {},
            "purged": not dry_run,
            "dry_run": dry_run,
            "details": [],
            "errors": [],
        }

        logger.info(
            f"{'[DRY RUN] ' if dry_run else ''}Scanning for corrupted cache entries"
            f"{f' for {symbol}' if symbol else ''}"
        )

        for cache_type, handlers in self.handlers.items():
            type_name = cache_type.value

            for handler in handlers:
                handler_name = handler.__class__.__name__

                try:
                    # Get entries to scan
                    if symbol:
                        if hasattr(handler, "list_entries_for_symbol"):
                            entries = handler.list_entries_for_symbol(symbol.upper())
                        else:
                            continue
                    else:
                        if hasattr(handler, "list_all_entries"):
                            entries = handler.list_all_entries()
                        else:
                            continue

                    for entry_key, entry_data in entries:
                        result["entries_scanned"] += 1

                        # Validate entry
                        is_valid, issues = self.validate_cache_entry(
                            entry_data, cache_type, strict=True
                        )

                        if not is_valid:
                            result["corrupted_entries"] += 1

                            # Categorize corruption types
                            for issue in issues:
                                # Extract corruption type from issue message
                                if "empty" in issue.lower():
                                    corruption_type = "empty_data"
                                elif "version" in issue.lower():
                                    corruption_type = "version_mismatch"
                                elif "null" in issue.lower():
                                    corruption_type = "null_values"
                                elif "missing" in issue.lower():
                                    corruption_type = "missing_fields"
                                elif "expired" in issue.lower():
                                    corruption_type = "expired"
                                else:
                                    corruption_type = "other"

                                result["corruption_types"][corruption_type] = (
                                    result["corruption_types"].get(corruption_type, 0) + 1
                                )

                            # Record detail
                            detail = {
                                "cache_type": type_name,
                                "handler": handler_name,
                                "key": str(entry_key)[:100],
                                "issues": issues,
                            }
                            result["details"].append(detail)

                            # Purge if not dry run
                            if not dry_run:
                                try:
                                    handler.delete(entry_key)
                                    logger.info(
                                        f"Purged corrupted entry [{handler_name}] "
                                        f"{type_name}: {entry_key}"
                                    )
                                except Exception as del_err:
                                    result["errors"].append(
                                        f"Failed to purge {entry_key}: {del_err}"
                                    )

                            logger.debug(
                                f"{'Would purge' if dry_run else 'Purged'} corrupted "
                                f"[{handler_name}] {type_name}: issues={issues}"
                            )
                        else:
                            result["healthy_entries"] += 1

                except Exception as e:
                    result["errors"].append(
                        f"Error scanning {handler_name}/{type_name}: {e}"
                    )
                    logger.error(f"Corruption scan error: {e}")

        total_time = (time.time() - operation_start) * 1000

        # Summary
        corruption_rate = (
            (result["corrupted_entries"] / result["entries_scanned"] * 100)
            if result["entries_scanned"] > 0 else 0
        )

        logger.info(
            f"{'[DRY RUN] ' if dry_run else ''}Corruption scan complete: "
            f"{result['corrupted_entries']}/{result['entries_scanned']} corrupted "
            f"({corruption_rate:.1f}%) | Time: {total_time:.1f}ms"
        )

        if result["corruption_types"]:
            logger.info(f"Corruption types: {result['corruption_types']}")

        return result


# Global cache manager instance
_cache_manager = None


def get_cache_manager() -> CacheManager:
    """Get or create global cache manager instance"""
    global _cache_manager
    if _cache_manager is None:
        _cache_manager = CacheManager()
    return _cache_manager
